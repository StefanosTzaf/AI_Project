{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83cd2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aaf65d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbf385",
   "metadata": {},
   "source": [
    "Roadmap:\n",
    "\n",
    "Training and inference:\n",
    " * Call RPN layers\n",
    " * Generate Anchors\n",
    " * Convert anchors to proposals using Box transformation prediction\n",
    " * Filter Proposals\n",
    "\n",
    "Training only:\n",
    " * Assign Ground Truth boxes to anchors\n",
    " * Compute labels and regression targets for anchors\n",
    " * Sample positive and negative anchors\n",
    " * Compute classification loss using sampled anchors\n",
    " * Compute localization loss using sampled positive anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18cbca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_positive_negative(labels, num_positive=256, num_negative=256):\n",
    "    \n",
    "    positive = torch.where(labels > 1)[0]\n",
    "    negative=torch.where(labels == 0)[0]\n",
    "    \n",
    "    num_pos=num_positive\n",
    "    num_pos=min(positive.numel(), num_pos)\n",
    "    \n",
    "    num_neg=num_negative \n",
    "    num_neg=min(negative.numel(), num_neg)\n",
    "    \n",
    "    perm_positive_ids=torch.randperm(positive.numel(), device=device)[:num_pos]\n",
    "    perm_negative_ids=torch.randperm(negative.numel(), device=device)[:num_neg]\n",
    "    \n",
    "    pos_ids=positive[perm_positive_ids]\n",
    "    neg_ids=negative[perm_negative_ids]\n",
    "    \n",
    "    sampled_pos_ids_mask = torch.zeros(labels.shape, dtype=torch.bool, device=device)\n",
    "    sampled_neg_ids_mask = torch.zeros(labels.shape, dtype=torch.bool, device=device)\n",
    "    \n",
    "    sampled_pos_ids_mask[pos_ids] = True\n",
    "    sampled_neg_ids_mask[neg_ids] = True    \n",
    "    \n",
    "    return sampled_pos_ids_mask, sampled_neg_ids_mask\n",
    "def get_IOU(box1, box2):\n",
    "\n",
    "\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1]) \n",
    "\n",
    "    # top left x1,y1 and bottom right x2,y2\n",
    "    x1 = torch.max(box1[:,None, 0], box2[:, 0])\n",
    "    y1 = torch.max(box1[:,None, 1], box2[:, 1])\n",
    "\n",
    "    x2 = torch.min(box1[:,None, 2], box2[:, 2])\n",
    "    y2 = torch.min(box1[:,None, 3], box2[:, 3])\n",
    "    \n",
    "    intersection = (x2-x1).clamp(min=0) * (y2-y1).clamp(min=0)\n",
    "    union = area1[:, None] + area2 - intersection\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "\n",
    "def box_to_boundary(boxes,img_shape):\n",
    "        boxes_x1 = boxes[..., 0]\n",
    "        boxes_y1 = boxes[..., 1]\n",
    "        boxes_x2 = boxes[..., 2]\n",
    "        boxes_y2 = boxes[..., 3]\n",
    "\n",
    "        height, width = img_shape[-2:]\n",
    "        boxes_x1 = torch.clamp(boxes_x1,max=width)\n",
    "        boxes_y1 = torch.clamp(boxes_y1,max=height)\n",
    "        boxes_x2 = torch.clamp(boxes_x2,max=width)\n",
    "        boxes_y2 = torch.clamp(boxes_y2,max=height)\n",
    "        boxes=torch.cat((\n",
    "            boxes_x1[..., None],\n",
    "            boxes_y1[..., None],\n",
    "            boxes_x2[..., None],\n",
    "            boxes_y2[..., None]\n",
    "        ),dim=-1)\n",
    "        return boxes\n",
    "class RegionalProposalNN(nn.Module):\n",
    "    def __init__(self, num_classes=8, in_channels=512):\n",
    "        \n",
    "        super(RegionalProposalNN, self).__init__()\n",
    "      \n",
    "\n",
    "\n",
    "        self.scales=[128, 256, 512]  \n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "\n",
    "        # 3x3 convolutional layer for RPN\n",
    "        self.rpn_conv = nn.Conv2d(in_channels,in_channels, kernel_size=3, padding=1,stride=1)\n",
    "        # 1x1 convolutional layer for classification\n",
    "        self.class_layer=nn.Conv2d(in_channels, self.num_anchors, kernel_size=1,stride=1)\n",
    "\n",
    "        # 1x1 convolutional layer for bounding box regression\n",
    "        self.bbox_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "\n",
    "    def anchors_to_predictions(self, predictions, anchors):\n",
    "        bbox_predictions = predictions.reshape(bbox_predictions.size(0), -1, 4)\n",
    "        \n",
    "        # Get xs, cy , w , h from the predictions (x1, y1, x2, y2)\n",
    "        w=anchors[:, 2] - anchors[:, 0]\n",
    "        h=anchors[:, 3] - anchors[:, 1]\n",
    "        cx = (anchors[:, 0] +  0.5 * w)\n",
    "        cy = (anchors[:, 1] + 0.5 * h)\n",
    "        dx= bbox_predictions[..., 0]\n",
    "        dy= bbox_predictions[..., 1]\n",
    "        dw= bbox_predictions[..., 2]\n",
    "        dh= bbox_predictions[..., 3]\n",
    "\n",
    "        pred_cx= dx * w[:,None] + cx[:, None]\n",
    "        pred_cy= dy * h[:,None] + cy[:, None]\n",
    "        pred_w = torch.exp(dw) * w[:, None]\n",
    "        pred_h = torch.exp(dh) * h[:, None]\n",
    "\n",
    "        pred_box_x1 = pred_cx - 0.5 * pred_w\n",
    "        pred_box_y1 = pred_cy - 0.5 * pred_h\n",
    "        pred_box_x2 = pred_cx + 0.5 * pred_w\n",
    "        pred_box_y2 = pred_cy + 0.5 * pred_h\n",
    "\n",
    "        pred_boxes= torch.stack([pred_box_x1, pred_box_y1, pred_box_x2, pred_box_y2], dim=2)\n",
    "\n",
    "        return pred_boxes\n",
    "\n",
    "\n",
    "    def transform_boxes_to_og_size(boxes,new_size,original_size):\n",
    "        ratios = [ torch.tensor(s_og, dtype=torch.float32,devices=boxes.device) / torch.tensor(s,dtype=torch.float32, devices=boxes.device) for s_og, s in zip(original_size, new_size)]\n",
    "\n",
    "        ratio_h, ratio_w = ratios\n",
    "        xmin,ymin, xmax,ymax = boxes.unbind(1)\n",
    "        xmin = xmin * ratio_w\n",
    "        ymin = ymin * ratio_h\n",
    "        xmax = xmax * ratio_w\n",
    "        ymax = ymax * ratio_h\n",
    "        return torch.stack([xmin, ymin, xmax, ymax], dim=1)\n",
    "\n",
    "    def generate_anchors(self, image,feature):\n",
    "        grid_h,grid_w=feature.shape[2],feature.shape[3]\n",
    "        image_h, image_w = image.shape[2], image.shape[3]\n",
    "        stride_h = image_h / grid_h\n",
    "        stride_w = image_w / grid_w\n",
    "\n",
    "\n",
    "        # Make sure h/w = aspect_ratio and hxw=1\n",
    "\n",
    "        h_ratios=torch.sqrt(aspect_ratios)\n",
    "        w_ratios=1/h_ratios\n",
    "\n",
    "        ws=(w_ratios[:,None] * self.scales[None,:]).view(-1)\n",
    "        hs=(h_ratios[:,None] * self.scales[None,:]).view(-1)\n",
    "        \n",
    "\n",
    "        base_anchors = (torch.stack([-ws,-hs,ws,hs], dim=1) /2 ).round()\n",
    "\n",
    "        # Get the shifts in the x and y axis\n",
    "\n",
    "        shifts_x = torch.arange(0, grid_w,device=feat.device) * stride_w\n",
    "        shifts_y = torch.arange(0, grid_h,device=feat.device) * stride_h\n",
    "\n",
    "        shifts_x, shifts_y = torch.meshgrid(shifts_x, shifts_y, indexing='ij')\n",
    "\n",
    "        shifts_x = shifts_x.reshape(-1)\n",
    "        shifts_y = shifts_y.reshape(-1)\n",
    "        shifts=torch.stack([shifts_x, shifts_y, shifts_x, shifts_y], dim=1)\n",
    "        anchors=(shifts.view(1, -1, 4) + base_anchors.view(-1, 1, 4))\n",
    "        anchors=anchors.reshape(-1,4)\n",
    "        return anchors\n",
    "    \n",
    "\n",
    "    def filter_proposals(self, proposals, class_scores, img_shape):\n",
    "        class_scores = class_scores.reshape(-1)\n",
    "        class_scores = torch.sigmoid(class_scores)\n",
    "        _, top_idx = torch.topk(class_scores, k=2000, sorted=True)\n",
    "        class_scores = class_scores[top_idx]\n",
    "        proposals = proposals[top_idx]\n",
    "        proposals = self.box_to_boundary(proposals, img_shape)\n",
    "\n",
    "        # NMS\n",
    "        keep_mask=torch.zeros_like(class_scores , dtype=torch.bool)\n",
    "        keep_ids= torchvision.ops.nms(proposals, class_scores, iou_threshold=0.7)             # IOU threshold 0.7\n",
    "        post_nms_keep_indexes = keep_ids[class_scores[keep_ids].sort(descending=True)][1]\n",
    "\n",
    "        # Post NMS filtering\n",
    "        proposals=proposals[post_nms_keep_indexes[:2000]]                                 # top 2000 proposals\n",
    "        class_scores = class_scores[post_nms_keep_indexes[:2000]]\n",
    "        return proposals, class_scores\n",
    "    \n",
    "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "        iou_matrix = get_IOU(anchors, gt_boxes)\n",
    "\n",
    "        # Get the best ground truth box for each anchor\n",
    "        best_match,best_gt_id = iou_matrix.max(dim=0)\n",
    "        best_gt_id_pre_treshold = best_gt_id.clone()         # jeep a copy of the best_gt_id before thresholding\n",
    "\n",
    "\n",
    "        below_threshold_mask = best_match < 0.3\n",
    "        between_threshold_mask = (best_match >= 0.3) & (best_match < 0.7)\n",
    "        best_gt_id[below_threshold_mask] = -1  # -1 for anchors that are below the threshold\n",
    "        best_gt_id[between_threshold_mask] = -2\n",
    "\n",
    "\n",
    "        # Low quality anchors\n",
    "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        gt_pred_pair_max_iou=torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "        \n",
    "        # Get all the anchor indexes\n",
    "        preds_ids_to_update=gt_pred_pair_max_iou[1]\n",
    "        best_gt_id[preds_ids_to_update]= best_gt_id_pre_treshold[best_gt_id_pre_treshold]\n",
    "\n",
    "        # Best match index is either valid or -1 or -2\n",
    "        matched_gt_boxes=gt_boxes[best_gt_id.clamp(min=0)]\n",
    "\n",
    "        # Set all  foreground anchors to 1 and background anchors to 0\n",
    "        labels = best_gt_id>=0\n",
    "        labels=labels.to(  torch.float32)\n",
    "\n",
    "        background_anchors= best_gt_id   == -1\n",
    "        labels[background_anchors] = 0.0\n",
    "\n",
    "        # anchors to be ignored to -1\n",
    "        ignore_anchors = best_gt_id == -2\n",
    "        labels[ignore_anchors] = -1.0\n",
    "\n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def boxes_to_transform_targets(self, groud_truth_boxes, anchors):\n",
    "        #Get center x,y h,w from x1, y1, x2, y2 for  anchors \n",
    "        widths= anchors[:, 2] - anchors[:, 0]\n",
    "        heights = anchors[:, 3] - anchors[:, 1]\n",
    "        cx = (anchors[:, 0] + 0.5 * widths)\n",
    "        cy = (anchors[:, 1] + 0.5 * heights)\n",
    "\n",
    "        # for gt boxes\n",
    "        gt_widths = groud_truth_boxes[:, 2] - groud_truth_boxes[:,0]\n",
    "        gt_heights = groud_truth_boxes[:, 3] - groud_truth_boxes[:,1]\n",
    "        gt_cx = (groud_truth_boxes[:, 0] + 0.5 * gt_widths)\n",
    "        gt_cy = (groud_truth_boxes[:, 1] + 0.5 * gt_heights)\n",
    "\n",
    "\n",
    "        target_dx = (gt_cx - cx) / widths\n",
    "        target_dy = (gt_cy - cy) / heights\n",
    "        target_dw = torch.log(gt_widths / widths)\n",
    "        target_dh = torch.log(gt_heights / heights)\n",
    "\n",
    "        regression_targets = torch.stack([target_dx, target_dy, target_dw, target_dh], dim=1)\n",
    "        return regression_targets\n",
    "\n",
    "    def forward(self, image, features,target):\n",
    "        rpn_feat=nn.ReLU()(self.rpn_conv(features))\n",
    "        classification_scores = self.class_layer(rpn_feat)\n",
    "        bbox_predictions = self.bbox_layer(rpn_feat)\n",
    "\n",
    "        anchors = self.generate_anchors(image, features)\n",
    "\n",
    "        # class_scores = (Batch,anchors per location, h_feat, w_feat)\n",
    "        anchors_per_location = classification_scores.shape[1]\n",
    "        classification_scores = classification_scores.permute(0, 2, 3,1)\n",
    "        classification_scores = classification_scores.reshape(-1,1)\n",
    "\n",
    "        # classs_scores= (Batch*H_feat*w_feat, anchors per location,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # bbox_predictions = (Batch,Anchors per location*4, h_feat, w_feat)\n",
    "        bbox_predictions = bbox_predictions.view(bbox_predictions.size(0),\n",
    "                                                 anchors_per_location,\n",
    "                                                 4,\n",
    "                                                 rpn_feat.shape[-2],\n",
    "                                                 rpn_feat.shape[-1])\n",
    "        bbox_predictions = bbox_predictions.permute(0, 3, 4, 1, 2)\n",
    "        bbox_predictions = bbox_predictions.reshape(-1, 4)\n",
    "        # bbox_predictions = (Batch*H_feat*w_feat, anchors per location,4)\n",
    "\n",
    "        proposals=self.anchors_to_predictions( (bbox_predictions.detach().reshape(-1,1,4), anchors))\n",
    "        proposals = proposals.reshape(proposals.size(0), 4)\n",
    "\n",
    "\n",
    "        proposals, class_scores = self.filter_proposals(proposals, classification_scores.detach(), image.shape)\n",
    "        rpn_output = {\n",
    "            'proposals': proposals,\n",
    "            'class_scores': class_scores\n",
    "        }\n",
    "        if not self.training or target is None:\n",
    "            return rpn_output\n",
    "        else:  # assign ground truth boxes and  labels to anchors\n",
    "\n",
    "            labels_for_anchors, matched_gt_boxes = self.assign_targets_to_anchors(anchors, target['boxes'][0])\n",
    "\n",
    "            regression_targets = self.boxes_to_transform_targets(matched_gt_boxes, anchors)\n",
    "\n",
    "            # Sample positive and negative anchors for training\n",
    "            sampled_pos_ids_mask, sampled_neg_ids_mask = sample_positive_negative(labels_for_anchors,128,128)\n",
    "\n",
    "            sampled_ids=torch.where(sampled_pos_ids_mask | sampled_neg_ids_mask)[0]\n",
    "            localization_loss  = nn.SmoothL1Loss(bbox_predictions[sampled_pos_ids_mask], regression_targets[sampled_ids],beta=1/9 , reductionn='sum') / sampled_ids.numel()\n",
    "\n",
    "            classification_loss=nn.binary_cross_entropy_with_logits(\n",
    "                classification_scores[sampled_ids].flatten(),\n",
    "                labels_for_anchors[sampled_ids].flatten(),\n",
    "                \n",
    "            ) \n",
    "\n",
    "            rpn_output['rpn_classificatoin_loss'] = classification_loss\n",
    "            rpn_output['rpn_localization_loss'] = localization_loss\n",
    "\n",
    "            return rpn_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cee62",
   "metadata": {},
   "source": [
    "ROI head road map:\n",
    "\n",
    "Training:\n",
    "* Assign ground truth boxes to proposals\n",
    "\n",
    "* Sample posotive and negative proposals\n",
    "* Get classification and regression targets for proposals\n",
    "* ROI pooling to get proposal features\n",
    "* Call classification and regression layers\n",
    "* Compute classification and localization loss\n",
    "\n",
    "Inference:\n",
    "* ROI pooling to get proposal features\n",
    "* Classification and regression\n",
    "* Convert proposals to predictions with box transformation prediction\n",
    "* Filter boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "368af11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIHead(nn.Module):\n",
    "    def __init__(self, num_classes=8, in_channels=512):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.pool_size = 7\n",
    "        self.fc_inner_dim = 1024\n",
    "\n",
    "        self.fc1=nn.Linear(in_channels*self.pool_size*self.pool_size, self.fc_inner_dim)\n",
    "\n",
    "        self.fc2=nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "        self.class_layer = nn.Linear(self.fc_inner_dim, num_classes)\n",
    "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, num_classes * 4)\n",
    "\n",
    "\n",
    "    def assign_targets_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        iou_matrix = get_IOU(proposals, gt_boxes)\n",
    "\n",
    "        # Get the best ground truth box for each proposal\n",
    "        best_match, best_gt_id = iou_matrix.max(dim=0)\n",
    "        \n",
    "        below_low_threshold_mask = best_match < 0.5\n",
    "        best_gt_id[below_low_threshold_mask] = -1  # -1 for proposals that are below the threshold\n",
    "        matched_gt_boxes = gt_boxes[best_gt_id.clamp(min=0)]\n",
    "        labels=gt_labels(best_gt_id.clamp(min=0))\n",
    "        labels=labels.to(torch.int64)\n",
    "\n",
    "        background_proposals = best_gt_id == -1\n",
    "        labels[background_proposals] = 0  # Background proposals are labeled as 0\n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def filter_predictions(self, pred_boxes,pred_labels,pred_scores):\n",
    "        # REmove low scoring boxes\n",
    "        keep_mask = torch.where(pred_scores > 0.05)[0]\n",
    "        pred_boxes,pred_scores,pred_labels = pred_boxes[keep_mask], pred_scores[keep_mask], pred_labels[keep_mask]\n",
    "\n",
    "        # NMS\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            ids= torch.where(pred_labels == class_id)[0]\n",
    "            keep_ids= torch.ops.torchvision.nms(\n",
    "                pred_boxes[ids], pred_scores[ids], iou_threshold=0.5)\n",
    "\n",
    "            keep_mask[ids[keep_ids]] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        post_nms_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
    "        keep=post_nms_indices[:100]  # Keep top 100 predictions\n",
    "        return pred_boxes[keep], pred_labels[keep], pred_scores[keep] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, features, proposals, image_shape,target):\n",
    "        if self.training and target is not None:\n",
    "            # Assign ground truth boxes to proposals\n",
    "            gt_boxes = target['boxes'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "\n",
    "            # Assign labels and gt boxes to proposals\n",
    "\n",
    "            labels,matched_gt_boxes = self.assign_targets_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "            \n",
    "            \n",
    "            sampled_neg_ids_mask, sampled_pos_ids_mask = sample_positive_negative(labels, 32, 128-32) \n",
    "            # Sample positive and negative proposals for training\n",
    "            \n",
    "            sampled_ids = torch.where(sampled_pos_ids_mask | sampled_neg_ids_mask)[0]\n",
    "\n",
    "            proposals = proposals[sampled_ids]\n",
    "            labels = labels[sampled_ids]\n",
    "            matched_gt_boxes = matched_gt_boxes[sampled_ids]\n",
    "\n",
    "            regression_targets = self.boxes_to_transform_targets(matched_gt_boxes, proposals)  \n",
    "\n",
    "        # ROI Pooling\n",
    "        spacial_scale= 0.0625\n",
    "\n",
    "        proposal_roi_pool_feats = torchvision.ops.roi_pool(\n",
    "            features, proposals, output_size=self.pool_size, spatial_scale=spacial_scale)\n",
    "        \n",
    "        \n",
    "        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
    "\n",
    "        box_fc1=torch.nn.functional.relu(self.fc1(proposal_roi_pool_feats))\n",
    "        box_fc2 = torch.nn.functional.relu(self.fc2(box_fc1))\n",
    "\n",
    "        class_scores = self.class_layer(box_fc2)\n",
    "        bbox_predictions = self.bbox_reg_layer(box_fc2)\n",
    "\n",
    "        num_boxes, num_classes = class_scores.shape\n",
    "        bbox_predictions = bbox_predictions.reshape(num_boxes, num_classes, 4)\n",
    "\n",
    "        frcnn_output = {}\n",
    "\n",
    "        if self.training and target is not None:\n",
    "            classification_loss = torch.nn.functional.cross_entropy(class_scores, labels)\n",
    "\n",
    "            #compute localization loss only for non background proposals\n",
    "            fg_proposal_ids=torch.where(labels > 0)[0]\n",
    "\n",
    "            fg_class_labels = labels[fg_proposal_ids]\n",
    "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                bbox_predictions[fg_proposal_ids, fg_class_labels],\n",
    "                regression_targets[fg_proposal_ids],\n",
    "                beta=1 / 9,\n",
    "                reduction='sum'\n",
    "            ) / fg_proposal_ids.numel()\n",
    "\n",
    "            frcnn_output['frcnn_classification_loss'] = classification_loss\n",
    "            frcnn_output['frcnn_localization_loss'] = localization_loss\n",
    "            return frcnn_output\n",
    "        else:\n",
    "            #apply transformation to the proposals\n",
    "            pred_boxes = apply_regression_to_proposals(bbox_predictions, proposals)\n",
    "\n",
    "            pred_scores = torch.nn.functional.softmax(class_scores, dim=1)\n",
    "\n",
    "            #clamp boxes to image boundaries\n",
    "            pred_boxes = box_to_boundary(pred_boxes, image_shape)\n",
    "\n",
    "            # create labels for predictions\n",
    "            pred_labels = torch.arrange(num_classes, device=pred_boxes.device)\n",
    "            pred_labels= pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "\n",
    "\n",
    "            # remove background predictions\n",
    "\n",
    "            pred_boxes = pred_boxes[:, 1:]\n",
    "            pred_scores = pred_scores[:, 1:]\n",
    "            pred_labels = pred_labels[:, 1:]\n",
    "\n",
    "            # batch everything making every class a separate prediction\n",
    "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "            pred_scores = pred_scores.reshape(-1)\n",
    "            pred_labels = pred_labels.reshape(-1)\n",
    "            \n",
    "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n",
    "            \n",
    "            frcnn_output['pred_boxes'] = pred_boxes\n",
    "            frcnn_output['pred_labels'] = pred_labels\n",
    "            frcnn_output['pred_scores'] = pred_scores\n",
    "\n",
    "            return frcnn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b71a0",
   "metadata": {},
   "source": [
    "The Faster RCNN is comprised of a pretrained vgg16 backbone and the regional proposal nn and roi head we implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3e26140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        vgg16=torchvision.models.vgg16(pretrained=True)\n",
    "        self.backbone = vgg16.features[:-1] # Exclude the last max pooling layer\n",
    "        self.rpn = RegionalProposalNN(num_classes=num_classes, in_channels=512)\n",
    "        self.roi_head = ROIHead(num_classes=num_classes, in_channels=512)\n",
    "\n",
    "        for layer in self.backbone[:10]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.image_mean= [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "        self.min_size = 600\n",
    "        self.max_size = 1000\n",
    "    def normalize_resize(self, image,bboxes=None):\n",
    "        # Normalize the image\n",
    "        mean= torch.as_tensor(self.image_mean, dtype=image.dtype, device=image.device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=image.dtype, device=image.device)\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "        # resize the image so that lower dim gets to 600 but larger dim does not exceed 1000\n",
    "\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape= torch.tensor(image.shape[-2:])\n",
    "        min_size = torch.min(im_shape).to(torch.float32)\n",
    "        max_size = torch.max(im_shape).to(torch.float32)\n",
    "        scale=torch.min(\n",
    "            float(self.min_size) / min_size,     \n",
    "            float(self.max_size) / max_size\n",
    "        )\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image,\n",
    "            scale_factor=scale,\n",
    "            mode='bilinear',\n",
    "            recompute_scale_factor=True, \n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        # Resize the bounding boxes if provided\n",
    "        if bboxes is not None:\n",
    "            ratios = [ torch.tensor(s,dtype=torch.float32,device=bboxes.device)/\n",
    "                      torch.tensor(s_orig,dtype=torch.float32,device=bboxes.device)\n",
    "                     for s, s_orig in zip(image.shape[-2:], (h,w))]\n",
    "            \n",
    "            ratio_h,ratio_w=ratios\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n",
    "            xmin = xmin * ratio_w\n",
    "            ymin = ymin * ratio_h\n",
    "            xmax = xmax * ratio_w\n",
    "            ymax = ymax * ratio_h\n",
    "\n",
    "            bboxes = torch.stack([xmin, ymin, xmax, ymax], dim=2)\n",
    "            return image, bboxes\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, image, target=None):\n",
    "        old_shape = image.shape[-2:]\n",
    "        if self.training:\n",
    "            image, bboxes = self.normalize_resize(image, target['bboxes'])\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            image, _ = self.normalize_resize(image,None)\n",
    "\n",
    "         # call backbone and RPN   \n",
    "        features = self.backbone(image)\n",
    "        rpn_output = self.rpn(image, features, target)\n",
    "        proposals = rpn_output['proposals']\n",
    "\n",
    "        FasterRCNN_output = self.roi_head(features, proposals, image.shape[-2:])\n",
    "\n",
    "        if not self.training:\n",
    "            # transform the predicted boxes to the original image shape\n",
    "            FasterRCNN_output['boxes'] = transform_boxes_to_og_size(\n",
    "                FasterRCNN_output['boxes'], image.shape[-2:], old_shape\n",
    "            )\n",
    "\n",
    "        return rpn_output, FasterRCNN_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40ba6a94",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Your code runs without errors. The previous cells have set up the environment and device.\n",
    "# You can start building your model or loading data here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298eb9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Combine all losses\u001b[39;00m\n\u001b[1;32m     54\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 61\u001b[0m, in \u001b[0;36mFasterRCNN.forward\u001b[0;34m(self, image, target)\u001b[0m\n\u001b[1;32m     59\u001b[0m old_shape \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m---> 61\u001b[0m     image, bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_resize(image, target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     62\u001b[0m     target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bboxes\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3da390",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'your_model_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming the model is already defined and imported\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myour_model_file\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FasterRCNN  \u001b[38;5;66;03m# Replace with your actual model module\u001b[39;00m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Dummy Image and Target Setup\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create one dummy image (3x224x224)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'your_model_file'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
