{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cd2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aaf65d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbf385",
   "metadata": {},
   "source": [
    "Roadmap:\n",
    "\n",
    "Training and inference:\n",
    " * Call RPN layers\n",
    " * Generate Anchors\n",
    " * Convert anchors to proposals using Box transformation prediction\n",
    " * Filter Proposals\n",
    "\n",
    "Training only:\n",
    " * Assign Ground Truth boxes to anchors\n",
    " * Compute labels and regression targets for anchors\n",
    " * Sample positive and negative anchors\n",
    " * Compute classification loss using sampled anchors\n",
    " * Compute localization loss using sampled positive anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18cbca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_positive_negative(labels, num_positive=256, num_negative=256):\n",
    "    \n",
    "    positive = torch.where(labels > 1)[0]\n",
    "    negative=torch.where(labels == 0)[0]\n",
    "    \n",
    "    num_pos=num_positive\n",
    "    num_pos=min(positive.numel(), num_pos)\n",
    "    \n",
    "    num_neg=num_negative \n",
    "    num_neg=min(negative.numel(), num_neg)\n",
    "    \n",
    "    perm_positive_ids=torch.randperm(positive.numel(), device=device)[:num_pos]\n",
    "    perm_negative_ids=torch.randperm(negative.numel(), device=device)[:num_neg]\n",
    "    \n",
    "    pos_ids=positive[perm_positive_ids]\n",
    "    neg_ids=negative[perm_negative_ids]\n",
    "    \n",
    "    sampled_pos_ids_mask = torch.zeros(labels.shape, dtype=torch.bool, device=device)\n",
    "    sampled_neg_ids_mask = torch.zeros(labels.shape, dtype=torch.bool, device=device)\n",
    "    \n",
    "    sampled_pos_ids_mask[pos_ids] = True\n",
    "    sampled_neg_ids_mask[neg_ids] = True    \n",
    "    \n",
    "    return sampled_pos_ids_mask, sampled_neg_ids_mask\n",
    "def get_IOU(box1, box2):\n",
    "\n",
    "\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1]) \n",
    "\n",
    "    # top left x1,y1 and bottom right x2,y2\n",
    "    x1 = torch.max(box1[:,None, 0], box2[:, 0])\n",
    "    y1 = torch.max(box1[:,None, 1], box2[:, 1])\n",
    "\n",
    "    x2 = torch.min(box1[:,None, 2], box2[:, 2])\n",
    "    y2 = torch.min(box1[:,None, 3], box2[:, 3])\n",
    "    \n",
    "    intersection = (x2-x1).clamp(min=0) * (y2-y1).clamp(min=0)\n",
    "    union = area1[:, None] + area2 - intersection\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "\n",
    "def box_to_boundary(boxes,img_shape):\n",
    "        boxes_x1 = boxes[..., 0]\n",
    "        boxes_y1 = boxes[..., 1]\n",
    "        boxes_x2 = boxes[..., 2]\n",
    "        boxes_y2 = boxes[..., 3]\n",
    "\n",
    "        height, width = img_shape[-2:]\n",
    "        boxes_x1 = torch.clamp(boxes_x1,max=width)\n",
    "        boxes_y1 = torch.clamp(boxes_y1,max=height)\n",
    "        boxes_x2 = torch.clamp(boxes_x2,max=width)\n",
    "        boxes_y2 = torch.clamp(boxes_y2,max=height)\n",
    "        boxes=torch.cat((\n",
    "            boxes_x1[..., None],\n",
    "            boxes_y1[..., None],\n",
    "            boxes_x2[..., None],\n",
    "            boxes_y2[..., None]\n",
    "        ),dim=-1)\n",
    "        return boxes\n",
    "class RegionalProposalNN(nn.Module):\n",
    "    def __init__(self, num_classes=8, in_channels=512):\n",
    "        \n",
    "        super(RegionalProposalNN, self).__init__()\n",
    "      \n",
    "\n",
    "\n",
    "        self.scales=[128, 256, 512]  \n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "\n",
    "        # 3x3 convolutional layer for RPN\n",
    "        self.rpn_conv = nn.Conv2d(in_channels,in_channels, kernel_size=3, padding=1,stride=1)\n",
    "        # 1x1 convolutional layer for classification\n",
    "        self.class_layer=nn.Conv2d(in_channels, self.num_anchors, kernel_size=1,stride=1)\n",
    "\n",
    "        # 1x1 convolutional layer for bounding box regression\n",
    "        self.bbox_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "\n",
    "    def anchors_to_predictions(self, predictions, anchors):\n",
    "        bbox_predictions = predictions.reshape(predictions.size(0), -1, 4)\n",
    "        \n",
    "        # Get xs, cy , w , h from the predictions (x1, y1, x2, y2)\n",
    "        w=anchors[:, 2] - anchors[:, 0]\n",
    "        h=anchors[:, 3] - anchors[:, 1]\n",
    "        cx = (anchors[:, 0] +  0.5 * w)\n",
    "        cy = (anchors[:, 1] + 0.5 * h)\n",
    "        dx= bbox_predictions[..., 0]\n",
    "        dy= bbox_predictions[..., 1]\n",
    "        dw= bbox_predictions[..., 2]\n",
    "        dh= bbox_predictions[..., 3]\n",
    "\n",
    "        pred_cx= dx * w[:,None] + cx[:, None]\n",
    "        pred_cy= dy * h[:,None] + cy[:, None]\n",
    "        pred_w = torch.exp(dw) * w[:, None]\n",
    "        pred_h = torch.exp(dh) * h[:, None]\n",
    "\n",
    "        pred_box_x1 = pred_cx - 0.5 * pred_w\n",
    "        pred_box_y1 = pred_cy - 0.5 * pred_h\n",
    "        pred_box_x2 = pred_cx + 0.5 * pred_w\n",
    "        pred_box_y2 = pred_cy + 0.5 * pred_h\n",
    "\n",
    "        pred_boxes= torch.stack([pred_box_x1, pred_box_y1, pred_box_x2, pred_box_y2], dim=2)\n",
    "\n",
    "        return pred_boxes\n",
    "\n",
    "\n",
    "    def transform_boxes_to_og_size(boxes,new_size,original_size):\n",
    "        ratios = [ torch.tensor(s_og, dtype=torch.float32,devices=boxes.device) / torch.tensor(s,dtype=torch.float32, devices=boxes.device) for s_og, s in zip(original_size, new_size)]\n",
    "\n",
    "        ratio_h, ratio_w = ratios\n",
    "        xmin,ymin, xmax,ymax = boxes.unbind(1)\n",
    "        xmin = xmin * ratio_w\n",
    "        ymin = ymin * ratio_h\n",
    "        xmax = xmax * ratio_w\n",
    "        ymax = ymax * ratio_h\n",
    "        return torch.stack([xmin, ymin, xmax, ymax], dim=1)\n",
    "\n",
    "    def generate_anchors(self, image,feature):\n",
    "        grid_h,grid_w=feature.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "\n",
    "        stride_h = torch.tensor(image_h//grid_h, dtype=torch.float32, device=feature.device)\n",
    "        stride_w = torch.tensor(image_w//grid_w, dtype=torch.float32, device=feature.device)\n",
    "        aspect_ratios = torch.tensor(self.aspect_ratios, dtype=torch.float32, device=feature.device)\n",
    "        scales = torch.tensor(self.scales, dtype=torch.float32, device=feature.device)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Make sure h/w = aspect_ratio and hxw=1\n",
    "\n",
    "        h_ratios=torch.sqrt(aspect_ratios)\n",
    "        w_ratios=1/h_ratios\n",
    "\n",
    "        ws=(w_ratios[:,None] * scales[None,:]).view(-1)\n",
    "        hs=(h_ratios[:,None] * scales[None,:]).view(-1)\n",
    "        \n",
    "\n",
    "        base_anchors = (torch.stack([-ws,-hs,ws,hs], dim=1) /2 ).round()\n",
    "\n",
    "        # Get the shifts in the x and y axis\n",
    "\n",
    "        shifts_x = torch.arange(0, grid_w,device=feature.device) * stride_w\n",
    "        shifts_y = torch.arange(0, grid_h,device=feature.device) * stride_h\n",
    "\n",
    "        shifts_x, shifts_y = torch.meshgrid(shifts_x, shifts_y, indexing='ij')\n",
    "\n",
    "        shifts_x = shifts_x.reshape(-1)\n",
    "        shifts_y = shifts_y.reshape(-1)\n",
    "        shifts=torch.stack([shifts_x, shifts_y, shifts_x, shifts_y], dim=1)\n",
    "        anchors=(shifts.view(1, -1, 4) + base_anchors.view(-1, 1, 4))\n",
    "        anchors=anchors.reshape(-1,4)\n",
    "        return anchors\n",
    "    \n",
    "\n",
    "    def filter_proposals(self, proposals, class_scores, img_shape):\n",
    "        class_scores = class_scores.reshape(-1)\n",
    "        class_scores = torch.sigmoid(class_scores)\n",
    "        _, top_idx = class_scores.topk(10000)\n",
    "        class_scores = class_scores[top_idx]\n",
    "        proposals = proposals[top_idx]\n",
    "        proposals = box_to_boundary(proposals, img_shape)\n",
    "\n",
    "        # NMS\n",
    "        keep_mask=torch.zeros_like(class_scores , dtype=torch.bool)\n",
    "        keep_ids= torch.ops.torchvision.nms(proposals, class_scores, iou_threshold=0.7)             # IOU threshold 0.7\n",
    "        post_nms_keep_indexes = keep_ids[class_scores[keep_ids].sort(descending=True)[1]]\n",
    "\n",
    "        # Post NMS filtering\n",
    "        proposals=proposals[post_nms_keep_indexes[:2000]]                                 # top 2000 proposals\n",
    "        class_scores = class_scores[post_nms_keep_indexes[:2000]]\n",
    "        return proposals, class_scores\n",
    "    \n",
    "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "        iou_matrix = get_IOU(anchors, gt_boxes)\n",
    "\n",
    "        # Get the best ground truth box for each anchor\n",
    "        best_match,best_gt_id = iou_matrix.max(dim=0)\n",
    "        best_gt_id_pre_treshold = best_gt_id.clone()         # jeep a copy of the best_gt_id before thresholding\n",
    "\n",
    "\n",
    "        below_threshold_mask = best_match < 0.3\n",
    "        between_threshold_mask = (best_match >= 0.3) & (best_match < 0.7)\n",
    "        best_gt_id[below_threshold_mask] = -1  # -1 for anchors that are below the threshold\n",
    "        best_gt_id[between_threshold_mask] = -2\n",
    "\n",
    "\n",
    "        # Low quality anchors\n",
    "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        gt_pred_pair_max_iou=torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "        \n",
    "        # Get all the anchor indexes\n",
    "        preds_ids_to_update=gt_pred_pair_max_iou[1]\n",
    "        best_gt_id[preds_ids_to_update]= best_gt_id_pre_treshold[preds_ids_to_update]\n",
    "\n",
    "        # Best match index is either valid or -1 or -2\n",
    "        matched_gt_boxes=gt_boxes[best_gt_id.clamp(min=0)]\n",
    "\n",
    "        # Set all  foreground anchors to 1 and background anchors to 0\n",
    "        labels = best_gt_id>=0\n",
    "        labels=labels.to(  torch.float32)\n",
    "\n",
    "        background_anchors= best_gt_id   == -1\n",
    "        labels[background_anchors] = 0.0\n",
    "\n",
    "        # anchors to be ignored to -1\n",
    "        ignore_anchors = best_gt_id == -2\n",
    "        labels[ignore_anchors] = -1.0\n",
    "\n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def boxes_to_transform_targets(self, groud_truth_boxes, anchors):\n",
    "        #Get center x,y h,w from x1, y1, x2, y2 for  anchors \n",
    "        widths= anchors[:, 2] - anchors[:, 0]\n",
    "        heights = anchors[:, 3] - anchors[:, 1]\n",
    "        cx = (anchors[:, 0] + 0.5 * widths)\n",
    "        cy = (anchors[:, 1] + 0.5 * heights)\n",
    "\n",
    "        # for gt boxes\n",
    "        gt_widths = groud_truth_boxes[:, 2] - groud_truth_boxes[:,0]\n",
    "        gt_heights = groud_truth_boxes[:, 3] - groud_truth_boxes[:,1]\n",
    "        gt_cx = (groud_truth_boxes[:, 0] + 0.5 * gt_widths)\n",
    "        gt_cy = (groud_truth_boxes[:, 1] + 0.5 * gt_heights)\n",
    "\n",
    "\n",
    "        target_dx = (gt_cx - cx) / widths\n",
    "        target_dy = (gt_cy - cy) / heights\n",
    "        target_dw = torch.log(gt_widths / widths)\n",
    "        target_dh = torch.log(gt_heights / heights)\n",
    "\n",
    "        regression_targets = torch.stack([target_dx, target_dy, target_dw, target_dh], dim=1)\n",
    "        return regression_targets\n",
    "\n",
    "    def forward(self, image, features,target):\n",
    "        rpn_feat=nn.ReLU()(self.rpn_conv(features))\n",
    "        classification_scores = self.class_layer(rpn_feat)\n",
    "        bbox_predictions = self.bbox_layer(rpn_feat)\n",
    "\n",
    "        anchors = self.generate_anchors(image, features)\n",
    "\n",
    "        # class_scores = (Batch,anchors per location, h_feat, w_feat)\n",
    "        anchors_per_location = classification_scores.shape[1]\n",
    "        classification_scores = classification_scores.permute(0, 2, 3,1)\n",
    "        classification_scores = classification_scores.reshape(-1,1)\n",
    "\n",
    "        # classs_scores= (Batch*H_feat*w_feat, anchors per location,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # bbox_predictions = (Batch,Anchors per location*4, h_feat, w_feat)\n",
    "        bbox_predictions = bbox_predictions.view(bbox_predictions.size(0),\n",
    "                                                 anchors_per_location,\n",
    "                                                 4,\n",
    "                                                 rpn_feat.shape[-2],\n",
    "                                                 rpn_feat.shape[-1])\n",
    "        bbox_predictions = bbox_predictions.permute(0, 3, 4, 1, 2)\n",
    "        bbox_predictions = bbox_predictions.reshape(-1, 4)\n",
    "        # bbox_predictions = (Batch*H_feat*w_feat, anchors per location,4)\n",
    "\n",
    "        proposals=self.anchors_to_predictions(bbox_predictions.detach().reshape(-1,1,4), anchors)\n",
    "        proposals = proposals.reshape(proposals.size(0), 4)\n",
    "\n",
    "\n",
    "        proposals, class_scores = self.filter_proposals(proposals, classification_scores.detach(), image.shape)\n",
    "        rpn_output = {\n",
    "            'proposals': proposals,\n",
    "            'class_scores': class_scores\n",
    "        }\n",
    "        if not self.training or target is None:\n",
    "            return rpn_output\n",
    "        else:  # assign ground truth boxes and  labels to anchors\n",
    "\n",
    "            labels_for_anchors, matched_gt_boxes = self.assign_targets_to_anchors(anchors, target['bboxes'][0])\n",
    "\n",
    "            regression_targets = self.boxes_to_transform_targets(matched_gt_boxes, anchors)\n",
    "\n",
    "            # Sample positive and negative anchors for training\n",
    "            sampled_pos_ids_mask, sampled_neg_ids_mask = sample_positive_negative(labels_for_anchors,128,128)\n",
    "\n",
    "            sampled_ids=torch.where(sampled_pos_ids_mask | sampled_neg_ids_mask)[0]\n",
    "            localization_loss  = nn.SmoothL1Loss(bbox_predictions[sampled_pos_ids_mask], regression_targets[sampled_ids],beta=1/9 , reductionn='sum') / sampled_ids.numel()\n",
    "\n",
    "            classification_loss=nn.binary_cross_entropy_with_logits(\n",
    "                classification_scores[sampled_ids].flatten(),\n",
    "                labels_for_anchors[sampled_ids].flatten(),\n",
    "                \n",
    "            ) \n",
    "\n",
    "            rpn_output['rpn_classificatoin_loss'] = classification_loss\n",
    "            rpn_output['rpn_localization_loss'] = localization_loss\n",
    "\n",
    "            return rpn_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cee62",
   "metadata": {},
   "source": [
    "ROI head road map:\n",
    "\n",
    "Training:\n",
    "* Assign ground truth boxes to proposals\n",
    "\n",
    "* Sample posotive and negative proposals\n",
    "* Get classification and regression targets for proposals\n",
    "* ROI pooling to get proposal features\n",
    "* Call classification and regression layers\n",
    "* Compute classification and localization loss\n",
    "\n",
    "Inference:\n",
    "* ROI pooling to get proposal features\n",
    "* Classification and regression\n",
    "* Convert proposals to predictions with box transformation prediction\n",
    "* Filter boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368af11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIHead(nn.Module):\n",
    "    def __init__(self, num_classes=8, in_channels=512):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.pool_size = 7\n",
    "        self.fc_inner_dim = 1024\n",
    "\n",
    "        self.fc1=nn.Linear(in_channels*self.pool_size*self.pool_size, self.fc_inner_dim)\n",
    "\n",
    "        self.fc2=nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "        self.class_layer = nn.Linear(self.fc_inner_dim, num_classes)\n",
    "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, num_classes * 4)\n",
    "\n",
    "\n",
    "    def assign_targets_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        iou_matrix = get_IOU(proposals, gt_boxes)\n",
    "\n",
    "        # Get the best ground truth box for each proposal\n",
    "        best_match, best_gt_id = iou_matrix.max(dim=0)\n",
    "        \n",
    "        below_low_threshold_mask = best_match < 0.5\n",
    "        best_gt_id[below_low_threshold_mask] = -1  # -1 for proposals that are below the threshold\n",
    "        matched_gt_boxes = gt_boxes[best_gt_id.clamp(min=0)]\n",
    "        labels=gt_labels(best_gt_id.clamp(min=0))\n",
    "        labels=labels.to(torch.int64)\n",
    "\n",
    "        background_proposals = best_gt_id == -1\n",
    "        labels[background_proposals] = 0  # Background proposals are labeled as 0\n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def filter_predictions(self, pred_boxes,pred_labels,pred_scores):\n",
    "        # REmove low scoring boxes\n",
    "        keep_mask = torch.where(pred_scores > 0.05)[0]\n",
    "        pred_boxes,pred_scores,pred_labels = pred_boxes[keep_mask], pred_scores[keep_mask], pred_labels[keep_mask]\n",
    "\n",
    "        # NMS\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            ids= torch.where(pred_labels == class_id)[0]\n",
    "            keep_ids= torch.ops.torchvision.nms(\n",
    "                pred_boxes[ids], pred_scores[ids], iou_threshold=0.5)\n",
    "\n",
    "            keep_mask[ids[keep_ids]] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        post_nms_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
    "        keep=post_nms_indices[:100]  # Keep top 100 predictions\n",
    "        return pred_boxes[keep], pred_labels[keep], pred_scores[keep] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, features, proposals, image_shape,target):\n",
    "        if self.training and target is not None:\n",
    "            # Assign ground truth boxes to proposals\n",
    "            gt_boxes = target['boxes'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "\n",
    "            # Assign labels and gt boxes to proposals\n",
    "\n",
    "            labels,matched_gt_boxes = self.assign_targets_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "            \n",
    "            \n",
    "            sampled_neg_ids_mask, sampled_pos_ids_mask = sample_positive_negative(labels, 32, 128-32) \n",
    "            # Sample positive and negative proposals for training\n",
    "            \n",
    "            sampled_ids = torch.where(sampled_pos_ids_mask | sampled_neg_ids_mask)[0]\n",
    "\n",
    "            proposals = proposals[sampled_ids]\n",
    "            labels = labels[sampled_ids]\n",
    "            matched_gt_boxes = matched_gt_boxes[sampled_ids]\n",
    "\n",
    "            regression_targets = self.boxes_to_transform_targets(matched_gt_boxes, proposals)  \n",
    "\n",
    "        # ROI Pooling\n",
    "        spacial_scale= 0.0625\n",
    "\n",
    "        proposal_roi_pool_feats = torchvision.ops.roi_pool(\n",
    "            features, proposals, output_size=self.pool_size, spatial_scale=spacial_scale)\n",
    "        \n",
    "        \n",
    "        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
    "\n",
    "        box_fc1=torch.nn.functional.relu(self.fc1(proposal_roi_pool_feats))\n",
    "        box_fc2 = torch.nn.functional.relu(self.fc2(box_fc1))\n",
    "\n",
    "        class_scores = self.class_layer(box_fc2)\n",
    "        bbox_predictions = self.bbox_reg_layer(box_fc2)\n",
    "\n",
    "        num_boxes, num_classes = class_scores.shape\n",
    "        bbox_predictions = bbox_predictions.reshape(num_boxes, num_classes, 4)\n",
    "\n",
    "        frcnn_output = {}\n",
    "\n",
    "        if self.training and target is not None:\n",
    "            classification_loss = torch.nn.functional.cross_entropy(class_scores, labels)\n",
    "\n",
    "            #compute localization loss only for non background proposals\n",
    "            fg_proposal_ids=torch.where(labels > 0)[0]\n",
    "\n",
    "            fg_class_labels = labels[fg_proposal_ids]\n",
    "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                bbox_predictions[fg_proposal_ids, fg_class_labels],\n",
    "                regression_targets[fg_proposal_ids],\n",
    "                beta=1 / 9,\n",
    "                reduction='sum'\n",
    "            ) / fg_proposal_ids.numel()\n",
    "\n",
    "            frcnn_output['frcnn_classification_loss'] = classification_loss\n",
    "            frcnn_output['frcnn_localization_loss'] = localization_loss\n",
    "            return frcnn_output\n",
    "        else:\n",
    "            #apply transformation to the proposals\n",
    "            pred_boxes = apply_regression_to_proposals(bbox_predictions, proposals)\n",
    "\n",
    "            pred_scores = torch.nn.functional.softmax(class_scores, dim=1)\n",
    "\n",
    "            #clamp boxes to image boundaries\n",
    "            pred_boxes = box_to_boundary(pred_boxes, image_shape)\n",
    "\n",
    "            # create labels for predictions\n",
    "            pred_labels = torch.arrange(num_classes, device=pred_boxes.device)\n",
    "            pred_labels= pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "\n",
    "\n",
    "            # remove background predictions\n",
    "\n",
    "            pred_boxes = pred_boxes[:, 1:]\n",
    "            pred_scores = pred_scores[:, 1:]\n",
    "            pred_labels = pred_labels[:, 1:]\n",
    "\n",
    "            # batch everything making every class a separate prediction\n",
    "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "            pred_scores = pred_scores.reshape(-1)\n",
    "            pred_labels = pred_labels.reshape(-1)\n",
    "            \n",
    "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n",
    "            \n",
    "            frcnn_output['pred_boxes'] = pred_boxes\n",
    "            frcnn_output['pred_labels'] = pred_labels\n",
    "            frcnn_output['pred_scores'] = pred_scores\n",
    "\n",
    "            return frcnn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b71a0",
   "metadata": {},
   "source": [
    "The Faster RCNN is comprised of a pretrained vgg16 backbone and the regional proposal nn and roi head we implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e26140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        vgg16=torchvision.models.vgg16(pretrained=True)\n",
    "        self.backbone = vgg16.features[:-1] # Exclude the last max pooling layer\n",
    "        self.rpn = RegionalProposalNN(num_classes=num_classes, in_channels=512)\n",
    "        self.roi_head = ROIHead(num_classes=num_classes, in_channels=512)\n",
    "\n",
    "        for layer in self.backbone[:10]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.image_mean= [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "        self.min_size = 600\n",
    "        self.max_size = 1000\n",
    "    def normalize_resize(self, image,bboxes=None):\n",
    "        # Normalize the image\n",
    "        mean= torch.as_tensor(self.image_mean, dtype=image.dtype, device=image.device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=image.dtype, device=image.device)\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "        # resize the image so that lower dim gets to 600 but larger dim does not exceed 1000\n",
    "\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape= torch.tensor(image.shape[-2:])\n",
    "        min_size = torch.min(im_shape).to(torch.float32)\n",
    "        max_size = torch.max(im_shape).to(torch.float32)\n",
    "        scale=torch.min(\n",
    "            float(self.min_size) / min_size,     \n",
    "            float(self.max_size) / max_size\n",
    "        )\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image,\n",
    "            scale_factor=scale,\n",
    "            mode='bilinear',\n",
    "            recompute_scale_factor=True, \n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        # Resize the bounding boxes if provided\n",
    "        if bboxes is not None:\n",
    "            ratios = [ torch.tensor(s,dtype=torch.float32,device=bboxes.device)/\n",
    "                      torch.tensor(s_orig,dtype=torch.float32,device=bboxes.device)\n",
    "                     for s, s_orig in zip(image.shape[-2:], (h,w))]\n",
    "            \n",
    "            ratio_h,ratio_w=ratios\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n",
    "            xmin = xmin * ratio_w\n",
    "            ymin = ymin * ratio_h\n",
    "            xmax = xmax * ratio_w\n",
    "            ymax = ymax * ratio_h\n",
    "\n",
    "            bboxes = torch.stack([xmin, ymin, xmax, ymax], dim=2)\n",
    "            return image, bboxes\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, image, target=None):\n",
    "        old_shape = image.shape[-2:]\n",
    "        if self.training:\n",
    "            image, bboxes = self.normalize_resize(image, target['bboxes'])\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            image, _ = self.normalize_resize(image,None)\n",
    "\n",
    "         # call backbone and RPN   \n",
    "        features = self.backbone(image)\n",
    "        rpn_output = self.rpn(image, features, target)\n",
    "        proposals = rpn_output['proposals']\n",
    "\n",
    "        FasterRCNN_output = self.roi_head(features, proposals, image.shape[-2:])\n",
    "\n",
    "        if not self.training:\n",
    "            # transform the predicted boxes to the original image shape\n",
    "            FasterRCNN_output['boxes'] = transform_boxes_to_og_size(\n",
    "                FasterRCNN_output['boxes'], image.shape[-2:], old_shape\n",
    "            )\n",
    "\n",
    "        return rpn_output, FasterRCNN_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330644de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_iou(boxes1, boxes2):\n",
    "    r\"\"\"\n",
    "    IOU between two sets of boxes\n",
    "    :param boxes1: (Tensor of shape N x 4)\n",
    "    :param boxes2: (Tensor of shape M x 4)\n",
    "    :return: IOU matrix of shape N x M\n",
    "    \"\"\"\n",
    "    # Area of boxes (x2-x1)*(y2-y1)\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n",
    "    \n",
    "    # Get top left x1,y1 coordinate\n",
    "    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n",
    "    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n",
    "    \n",
    "    # Get bottom right x2,y2 coordinate\n",
    "    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n",
    "    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n",
    "    \n",
    "    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n",
    "    union = area1[:, None] + area2 - intersection_area  # (N, M)\n",
    "    iou = intersection_area / union  # (N, M)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Given all anchor boxes or proposals in image and their respective\n",
    "    ground truth assignments, we use the x1,y1,x2,y2 coordinates of them\n",
    "    to get tx,ty,tw,th transformation targets for all anchor boxes or proposals\n",
    "    :param ground_truth_boxes: (anchors_or_proposals_in_image, 4)\n",
    "        Ground truth box assignments for the anchors/proposals\n",
    "    :param anchors_or_proposals: (anchors_or_proposals_in_image, 4) Anchors/Proposal boxes\n",
    "    :return: regression_targets: (anchors_or_proposals_in_image, 4) transformation targets tx,ty,tw,th\n",
    "        for all anchors/proposal boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for anchors\n",
    "    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n",
    "    \n",
    "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for gt boxes\n",
    "    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
    "    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
    "    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n",
    "    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n",
    "    \n",
    "    targets_dx = (gt_center_x - center_x) / widths\n",
    "    targets_dy = (gt_center_y - center_y) / heights\n",
    "    targets_dw = torch.log(gt_widths / widths)\n",
    "    targets_dh = torch.log(gt_heights / heights)\n",
    "    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
    "    return regression_targets\n",
    "\n",
    "\n",
    "def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Given the transformation parameter predictions for all\n",
    "    input anchors or proposals, transform them accordingly\n",
    "    to generate predicted proposals or predicted boxes\n",
    "    :param box_transform_pred: (num_anchors_or_proposals, num_classes, 4)\n",
    "    :param anchors_or_proposals: (num_anchors_or_proposals, 4)\n",
    "    :return pred_boxes: (num_anchors_or_proposals, num_classes, 4)\n",
    "    \"\"\"\n",
    "    box_transform_pred = box_transform_pred.reshape(\n",
    "        box_transform_pred.size(0), -1, 4)\n",
    "    \n",
    "    # Get cx, cy, w, h from x1,y1,x2,y2\n",
    "    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n",
    "    \n",
    "    dx = box_transform_pred[..., 0]\n",
    "    dy = box_transform_pred[..., 1]\n",
    "    dw = box_transform_pred[..., 2]\n",
    "    dh = box_transform_pred[..., 3]\n",
    "    # dh -> (num_anchors_or_proposals, num_classes)\n",
    "    \n",
    "    # Prevent sending too large values into torch.exp()\n",
    "    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n",
    "    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n",
    "    \n",
    "    pred_center_x = dx * w[:, None] + center_x[:, None]\n",
    "    pred_center_y = dy * h[:, None] + center_y[:, None]\n",
    "    pred_w = torch.exp(dw) * w[:, None]\n",
    "    pred_h = torch.exp(dh) * h[:, None]\n",
    "    # pred_center_x -> (num_anchors_or_proposals, num_classes)\n",
    "    \n",
    "    pred_box_x1 = pred_center_x - 0.5 * pred_w\n",
    "    pred_box_y1 = pred_center_y - 0.5 * pred_h\n",
    "    pred_box_x2 = pred_center_x + 0.5 * pred_w\n",
    "    pred_box_y2 = pred_center_y + 0.5 * pred_h\n",
    "    \n",
    "    pred_boxes = torch.stack((\n",
    "        pred_box_x1,\n",
    "        pred_box_y1,\n",
    "        pred_box_x2,\n",
    "        pred_box_y2),\n",
    "        dim=2)\n",
    "    # pred_boxes -> (num_anchors_or_proposals, num_classes, 4)\n",
    "    return pred_boxes\n",
    "\n",
    "\n",
    "def sample_positive_negative(labels, positive_count, total_count):\n",
    "    # Sample positive and negative proposals\n",
    "    positive = torch.where(labels >= 1)[0]\n",
    "    negative = torch.where(labels == 0)[0]\n",
    "    num_pos = positive_count\n",
    "    num_pos = min(positive.numel(), num_pos)\n",
    "    num_neg = total_count - num_pos\n",
    "    num_neg = min(negative.numel(), num_neg)\n",
    "    perm_positive_idxs = torch.randperm(positive.numel(),\n",
    "                                        device=positive.device)[:num_pos]\n",
    "    perm_negative_idxs = torch.randperm(negative.numel(),\n",
    "                                        device=negative.device)[:num_neg]\n",
    "    pos_idxs = positive[perm_positive_idxs]\n",
    "    neg_idxs = negative[perm_negative_idxs]\n",
    "    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_pos_idx_mask[pos_idxs] = True\n",
    "    sampled_neg_idx_mask[neg_idxs] = True\n",
    "    return sampled_neg_idx_mask, sampled_pos_idx_mask\n",
    "\n",
    "\n",
    "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
    "    boxes_x1 = boxes[..., 0]\n",
    "    boxes_y1 = boxes[..., 1]\n",
    "    boxes_x2 = boxes[..., 2]\n",
    "    boxes_y2 = boxes[..., 3]\n",
    "    height, width = image_shape[-2:]\n",
    "    boxes_x1 = boxes_x1.clamp(min=0, max=width)\n",
    "    boxes_x2 = boxes_x2.clamp(min=0, max=width)\n",
    "    boxes_y1 = boxes_y1.clamp(min=0, max=height)\n",
    "    boxes_y2 = boxes_y2.clamp(min=0, max=height)\n",
    "    boxes = torch.cat((\n",
    "        boxes_x1[..., None],\n",
    "        boxes_y1[..., None],\n",
    "        boxes_x2[..., None],\n",
    "        boxes_y2[..., None]),\n",
    "        dim=-1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def transform_boxes_to_original_size(boxes, new_size, original_size):\n",
    "    r\"\"\"\n",
    "    Boxes are for resized image (min_size=600, max_size=1000).\n",
    "    This method converts the boxes to whatever dimensions\n",
    "    the image was before resizing\n",
    "    :param boxes:\n",
    "    :param new_size:\n",
    "    :param original_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ratios = [\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_height, ratio_width = ratios\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    xmin = xmin * ratio_width\n",
    "    xmax = xmax * ratio_width\n",
    "    ymin = ymin * ratio_height\n",
    "    ymax = ymax * ratio_height\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    r\"\"\"\n",
    "    RPN with following layers on the feature map\n",
    "        1. 3x3 conv layer followed by Relu\n",
    "        2. 1x1 classification conv with num_anchors(num_scales x num_aspect_ratios) output channels\n",
    "        3. 1x1 classification conv with 4 x num_anchors output channels\n",
    "\n",
    "    Classification is done via one value indicating probability of foreground\n",
    "    with sigmoid applied during inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.low_iou_threshold = model_config['rpn_bg_threshold']\n",
    "        self.high_iou_threshold = model_config['rpn_fg_threshold']\n",
    "        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n",
    "        self.rpn_batch_size = model_config['rpn_batch_size']\n",
    "        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n",
    "        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n",
    "        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n",
    "            else model_config['rpn_test_prenms_topk']\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "        \n",
    "        # 3x3 conv layer\n",
    "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 1x1 classification conv layer\n",
    "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
    "        \n",
    "        # 1x1 regression\n",
    "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "        \n",
    "        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n",
    "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
    "            torch.nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "    def generate_anchors(self, image, feat):\n",
    "        r\"\"\"\n",
    "        Method to generate anchors. First we generate one set of zero-centred anchors\n",
    "        using the scales and aspect ratios provided.\n",
    "        We then generate shift values in x,y axis for all featuremap locations.\n",
    "        The single zero centred anchors generated are replicated and shifted accordingly\n",
    "        to generate anchors for all feature map locations.\n",
    "        Note that these anchors are generated such that their centre is top left corner of the\n",
    "        feature map cell rather than the centre of the feature map cell.\n",
    "        :param image: (N, C, H, W) tensor\n",
    "        :param feat: (N, C_feat, H_feat, W_feat) tensor\n",
    "        :return: anchor boxes of shape (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        \"\"\"\n",
    "        grid_h, grid_w = feat.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "        \n",
    "        # For the vgg16 case stride would be 16 for both h and w\n",
    "        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n",
    "        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n",
    "        \n",
    "        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
    "        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
    "        \n",
    "        # Assuming anchors of scale 128 sq pixels\n",
    "        # For 1:1 it would be (128, 128) -> area=16384\n",
    "        # For 2:1 it would be (181.02, 90.51) -> area=16384\n",
    "        # For 1:2 it would be (90.51, 181.02) -> area=16384\n",
    "        \n",
    "        # The below code ensures h/w = aspect_ratios and h*w=1\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "        \n",
    "        # Now we will just multiply h and w with scale(example 128)\n",
    "        # to make h*w = 128 sq pixels and h/w = aspect_ratios\n",
    "        # This gives us the widths and heights of all anchors\n",
    "        # which we need to replicate at all locations\n",
    "        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        \n",
    "        # Now we make all anchors zero centred\n",
    "        # So x1, y1, x2, y2 = -w/2, -h/2, w/2, h/2\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "        base_anchors = base_anchors.round()\n",
    "        \n",
    "        # Get the shifts in x axis (0, 1,..., W_feat-1) * stride_w\n",
    "        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
    "\n",
    "        # Get the shifts in x axis (0, 1,..., H_feat-1) * stride_h\n",
    "        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
    "        \n",
    "        # Create a grid using these shifts\n",
    "        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
    "        # shifts_x -> (H_feat, W_feat)\n",
    "        # shifts_y -> (H_feat, W_feat)\n",
    "        \n",
    "        shifts_x = shifts_x.reshape(-1)\n",
    "        shifts_y = shifts_y.reshape(-1)\n",
    "        # Setting shifts for x1 and x2(same as shifts_x) and y1 and y2(same as shifts_y)\n",
    "        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        \n",
    "        # base_anchors -> (num_anchors_per_location, 4)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        # Add these shifts to each of the base anchors\n",
    "        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n",
    "        # anchors -> (H_feat * W_feat, num_anchors_per_location, 4)\n",
    "        anchors = anchors.reshape(-1, 4)\n",
    "        # anchors -> (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        return anchors\n",
    "    \n",
    "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "        r\"\"\"\n",
    "        For each anchor assign a ground truth box based on the IOU.\n",
    "        Also creates classification labels to be used for training\n",
    "        label=1 for anchors where maximum IOU with a gtbox > high_iou_threshold\n",
    "        label=0 for anchors where maximum IOU with a gtbox < low_iou_threshold\n",
    "        label=-1 for anchors where maximum IOU with a gtbox between (low_iou_threshold, high_iou_threshold)\n",
    "        :param anchors: (num_anchors_in_image, 4) all anchor boxes\n",
    "        :param gt_boxes: (num_gt_boxes_in_image, 4) all ground truth boxes\n",
    "        :return:\n",
    "            label: (num_anchors_in_image) {-1/0/1}\n",
    "            matched_gt_boxes: (num_anchors_in_image, 4) coordinates of assigned gt_box to each anchor\n",
    "                Even background/to_be_ignored anchors will be assigned some ground truth box.\n",
    "                It's fine, we will use label to differentiate those instances later\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get (gt_boxes, num_anchors_in_image) IOU matrix\n",
    "        iou_matrix = get_iou(gt_boxes, anchors)\n",
    "        \n",
    "        # For each anchor get the gt box index with maximum overlap\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        # best_match_gt_idx -> (num_anchors_in_image)\n",
    "        \n",
    "        # This copy of best_match_gt_idx will be needed later to\n",
    "        # add low quality matches\n",
    "        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n",
    "        \n",
    "        # Based on threshold, update the values of best_match_gt_idx\n",
    "        # For anchors with highest IOU < low_threshold update to be -1\n",
    "        # For anchors with highest IOU between low_threshold & high threshold update to be -2\n",
    "        below_low_threshold = best_match_iou < self.low_iou_threshold\n",
    "        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n",
    "        best_match_gt_idx[below_low_threshold] = -1\n",
    "        best_match_gt_idx[between_thresholds] = -2\n",
    "        \n",
    "        # Add low quality anchor boxes, if for a given ground truth box, these are the ones\n",
    "        # that have highest IOU with that gt box\n",
    "        \n",
    "        # For each gt box, get the maximum IOU value amongst all anchors\n",
    "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        # best_anchor_iou_for_gt -> (num_gt_boxes_in_image)\n",
    "        \n",
    "        # For each gt box get those anchors\n",
    "        # which have this same IOU as present in best_anchor_iou_for_gt\n",
    "        # This is to ensure if 10 anchors all have the same IOU value,\n",
    "        # which is equal to the highest IOU that this gt box has with any anchor\n",
    "        # then we get all these 10 anchors\n",
    "        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "        # gt_pred_pair_with_highest_iou -> [0, 0, 0, 1, 1, 1], [8896,  8905,  8914, 10472, 10805, 11138]\n",
    "        # This means that anchors at the first 3 indexes have an IOU with gt box at index 0\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # Similarly anchor at last three indexes(10472, 10805, 11138) have an IOU with gt box at index 1\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # These 6 anchor indexes will also be added as positive anchors\n",
    "        \n",
    "        # Get all the anchors indexes to update\n",
    "        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n",
    "        \n",
    "        # Update the matched gt index for all these anchors with whatever was the best gt box\n",
    "        # prior to thresholding\n",
    "        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n",
    "        \n",
    "        # best_match_gt_idx is either a valid index for all anchors or -1(background) or -2(to be ignored)\n",
    "        # Clamp this so that the best_match_gt_idx is a valid non-negative index\n",
    "        # At this moment the -1 and -2 labelled anchors will be mapped to the 0th gt box\n",
    "        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Set all foreground anchor labels as 1\n",
    "        labels = best_match_gt_idx >= 0\n",
    "        labels = labels.to(dtype=torch.float32)\n",
    "        \n",
    "        # Set all background anchor labels as 0\n",
    "        background_anchors = best_match_gt_idx == -1\n",
    "        labels[background_anchors] = 0.0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1\n",
    "        ignored_anchors = best_match_gt_idx == -2\n",
    "        labels[ignored_anchors] = -1.0\n",
    "        # Later for classification we will only pick labels which have > 0 label\n",
    "        \n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "        r\"\"\"\n",
    "        This method does three kinds of filtering/modifications\n",
    "        1. Pre NMS topK filtering\n",
    "        2. Make proposals valid by clamping coordinates(0, width/height)\n",
    "        2. Small Boxes filtering based on width and height\n",
    "        3. NMS\n",
    "        4. Post NMS topK filtering\n",
    "        :param proposals: (num_anchors_in_image, 4)\n",
    "        :param cls_scores: (num_anchors_in_image, 4) these are cls logits\n",
    "        :param image_shape: resized image shape needed to clip proposals to image boundary\n",
    "        :return: proposals and cls_scores: (num_filtered_proposals, 4) and (num_filtered_proposals)\n",
    "        \"\"\"\n",
    "        # Pre NMS Filtering\n",
    "        cls_scores = cls_scores.reshape(-1)\n",
    "        cls_scores = torch.sigmoid(cls_scores)\n",
    "        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n",
    "        \n",
    "        cls_scores = cls_scores[top_n_idx]\n",
    "        proposals = proposals[top_n_idx]\n",
    "        ##################\n",
    "        \n",
    "        # Clamp boxes to image boundary\n",
    "        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n",
    "        ####################\n",
    "        \n",
    "        # Small boxes based on width and height filtering\n",
    "        min_size = 16\n",
    "        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        proposals = proposals[keep]\n",
    "        cls_scores = cls_scores[keep]\n",
    "        ####################\n",
    "        \n",
    "        # NMS based on objectness scores\n",
    "        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n",
    "        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n",
    "        keep_mask[keep_indices] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        # Sort by objectness\n",
    "        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
    "        \n",
    "        # Post NMS topk filtering\n",
    "        proposals, cls_scores = (proposals[post_nms_keep_indices[:self.rpn_topk]],\n",
    "                                 cls_scores[post_nms_keep_indices[:self.rpn_topk]])\n",
    "        \n",
    "        return proposals, cls_scores\n",
    "    \n",
    "    def forward(self, image, feat, target=None):\n",
    "        r\"\"\"\n",
    "        Main method for RPN does the following:\n",
    "        1. Call RPN specific conv layers to generate classification and\n",
    "            bbox transformation predictions for anchors\n",
    "        2. Generate anchors for entire image\n",
    "        3. Transform generated anchors based on predicted bbox transformation to generate proposals\n",
    "        4. Filter proposals\n",
    "        5. For training additionally we do the following:\n",
    "            a. Assign target ground truth labels and boxes to each anchors\n",
    "            b. Sample positive and negative anchors\n",
    "            c. Compute classification loss using sampled pos/neg anchors\n",
    "            d. Compute Localization loss using sampled pos anchors\n",
    "        :param image:\n",
    "        :param feat:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Call RPN layers\n",
    "        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n",
    "        cls_scores = self.cls_layer(rpn_feat)\n",
    "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "        # Generate anchors\n",
    "        anchors = self.generate_anchors(image, feat)\n",
    "        \n",
    "        # Reshape classification scores to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 1)\n",
    "        # cls_score -> (Batch_Size, Number of Anchors per location, H_feat, W_feat)\n",
    "        number_of_anchors_per_location = cls_scores.size(1)\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1)\n",
    "        cls_scores = cls_scores.reshape(-1, 1)\n",
    "        # cls_score -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 1)\n",
    "        \n",
    "        # Reshape bbox predictions to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 4)\n",
    "        # box_transform_pred -> (Batch_Size, Number of Anchors per location*4, H_feat, W_feat)\n",
    "        box_transform_pred = box_transform_pred.view(\n",
    "            box_transform_pred.size(0),\n",
    "            number_of_anchors_per_location,\n",
    "            4,\n",
    "            rpn_feat.shape[-2],\n",
    "            rpn_feat.shape[-1])\n",
    "        box_transform_pred = box_transform_pred.permute(0, 3, 4, 1, 2)\n",
    "        box_transform_pred = box_transform_pred.reshape(-1, 4)\n",
    "        # box_transform_pred -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 4)\n",
    "        \n",
    "        # Transform generated anchors according to box transformation prediction\n",
    "        proposals = apply_regression_pred_to_anchors_or_proposals(\n",
    "            box_transform_pred.detach().reshape(-1, 1, 4),\n",
    "            anchors)\n",
    "        proposals = proposals.reshape(proposals.size(0), 4)\n",
    "        ######################\n",
    "        \n",
    "        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n",
    "        rpn_output = {\n",
    "            'proposals': proposals,\n",
    "            'scores': scores\n",
    "        }\n",
    "        if not self.training or target is None:\n",
    "            # If we are not training no need to do anything\n",
    "            return rpn_output\n",
    "        else:\n",
    "            # Assign gt box and label for each anchor\n",
    "            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n",
    "                anchors,\n",
    "                target['bboxes'][0])\n",
    "            \n",
    "            # Based on gt assignment above, get regression target for the anchors\n",
    "            # matched_gt_boxes_for_anchors -> (Number of anchors in image, 4)\n",
    "            # anchors -> (Number of anchors in image, 4)\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n",
    "            \n",
    "            ####### Sampling positive and negative anchors ####\n",
    "            # Our labels were {fg:1, bg:0, to_be_ignored:-1}\n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
    "                labels_for_anchors,\n",
    "                positive_count=self.rpn_pos_count,\n",
    "                total_count=self.rpn_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            localization_loss = (\n",
    "                    torch.nn.functional.smooth_l1_loss(\n",
    "                        box_transform_pred[sampled_pos_idx_mask],\n",
    "                        regression_targets[sampled_pos_idx_mask],\n",
    "                        beta=1 / 9,\n",
    "                        reduction=\"sum\",\n",
    "                    )\n",
    "                    / (sampled_idxs.numel())\n",
    "            ) \n",
    "\n",
    "            cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(cls_scores[sampled_idxs].flatten(),\n",
    "                                                                            labels_for_anchors[sampled_idxs].flatten())\n",
    "\n",
    "            rpn_output['rpn_classification_loss'] = cls_loss\n",
    "            rpn_output['rpn_localization_loss'] = localization_loss\n",
    "            return rpn_output\n",
    "\n",
    "\n",
    "class ROIHead(nn.Module):\n",
    "    r\"\"\"\n",
    "    ROI head on top of ROI pooling layer for generating\n",
    "    classification and box transformation predictions\n",
    "    We have two fc layers followed by a classification fc layer\n",
    "    and a bbox regression fc layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_config, num_classes, in_channels):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.roi_batch_size = model_config['roi_batch_size']\n",
    "        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n",
    "        self.iou_threshold = model_config['roi_iou_threshold']\n",
    "        self.low_bg_iou = model_config['roi_low_bg_iou']\n",
    "        self.nms_threshold = model_config['roi_nms_threshold']\n",
    "        self.topK_detections = model_config['roi_topk_detections']\n",
    "        self.low_score_threshold = model_config['roi_score_threshold']\n",
    "        self.pool_size = model_config['roi_pool_size']\n",
    "        self.fc_inner_dim = model_config['fc_inner_dim']\n",
    "        \n",
    "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
    "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
    "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
    "        \n",
    "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
    "        torch.nn.init.constant_(self.cls_layer.bias, 0)\n",
    "\n",
    "        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n",
    "        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n",
    "    \n",
    "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        r\"\"\"\n",
    "        Given a set of proposals and ground truth boxes and their respective labels.\n",
    "        Use IOU to assign these proposals to some gt box or background\n",
    "        :param proposals: (number_of_proposals, 4)\n",
    "        :param gt_boxes: (number_of_gt_boxes, 4)\n",
    "        :param gt_labels: (number_of_gt_boxes)\n",
    "        :return:\n",
    "            labels: (number_of_proposals)\n",
    "            matched_gt_boxes: (number_of_proposals, 4)\n",
    "        \"\"\"\n",
    "        # Get IOU Matrix between gt boxes and proposals\n",
    "        iou_matrix = get_iou(gt_boxes, proposals)\n",
    "        # For each gt box proposal find best matching gt box\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n",
    "        ignored_proposals = best_match_iou < self.low_bg_iou\n",
    "        \n",
    "        # Update best match of low IOU proposals to -1\n",
    "        best_match_gt_idx[background_proposals] = -1\n",
    "        best_match_gt_idx[ignored_proposals] = -2\n",
    "        \n",
    "        # Get best marching gt boxes for ALL proposals\n",
    "        # Even background proposals would have a gt box assigned to it\n",
    "        # Label will be used to ignore them later\n",
    "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Get class label for all proposals according to matching gt boxes\n",
    "        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n",
    "        labels = labels.to(dtype=torch.int64)\n",
    "        \n",
    "        # Update background proposals to be of label 0(background)\n",
    "        labels[background_proposals] = 0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1(will be ignored)\n",
    "        labels[ignored_proposals] = -1\n",
    "        \n",
    "        return labels, matched_gt_boxes_for_proposals\n",
    "    \n",
    "    def forward(self, feat, proposals, image_shape, target):\n",
    "        r\"\"\"\n",
    "        Main method for ROI head that does the following:\n",
    "        1. If training assign target boxes and labels to all proposals\n",
    "        2. If training sample positive and negative proposals\n",
    "        3. If training get bbox transformation targets for all proposals based on assignments\n",
    "        4. Get ROI Pooled features for all proposals\n",
    "        5. Call fc6, fc7 and classification and bbox transformation fc layers\n",
    "        6. Compute classification and localization loss\n",
    "\n",
    "        :param feat:\n",
    "        :param proposals:\n",
    "        :param image_shape:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.training and target is not None:\n",
    "            # Add ground truth to proposals\n",
    "            proposals = torch.cat([proposals, target['bboxes'][0]], dim=0)\n",
    "            \n",
    "            gt_boxes = target['bboxes'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "            \n",
    "            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "            \n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(labels,\n",
    "                                                                                  positive_count=self.roi_pos_count,\n",
    "                                                                                  total_count=self.roi_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            # Keep only sampled proposals\n",
    "            proposals = proposals[sampled_idxs]\n",
    "            labels = labels[sampled_idxs]\n",
    "            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_proposals, proposals)\n",
    "            # regression_targets -> (sampled_training_proposals, 4)\n",
    "            # matched_gt_boxes_for_proposals -> (sampled_training_proposals, 4)\n",
    "        \n",
    "        # Get desired scale to pass to roi pooling function\n",
    "        # For vgg16 case this would be 1/16 (0.0625)\n",
    "        size = feat.shape[-2:]\n",
    "        possible_scales = []\n",
    "        for s1, s2 in zip(size, image_shape):\n",
    "            approx_scale = float(s1) / float(s2)\n",
    "            scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n",
    "            possible_scales.append(scale)\n",
    "        assert possible_scales[0] == possible_scales[1]\n",
    "        \n",
    "        # ROI pooling and call all layers for prediction\n",
    "        proposal_roi_pool_feats = torchvision.ops.roi_pool(feat, [proposals],\n",
    "                                                           output_size=self.pool_size,\n",
    "                                                           spatial_scale=possible_scales[0])\n",
    "        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
    "        box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_pool_feats))\n",
    "        box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n",
    "        cls_scores = self.cls_layer(box_fc_7)\n",
    "        box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
    "        # cls_scores -> (proposals, num_classes)\n",
    "        # box_transform_pred -> (proposals, num_classes * 4)\n",
    "        ##############################################\n",
    "        \n",
    "        num_boxes, num_classes = cls_scores.shape\n",
    "        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n",
    "        frcnn_output = {}\n",
    "        if self.training and target is not None:\n",
    "            classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n",
    "            \n",
    "            # Compute localization loss only for non-background labelled proposals\n",
    "            fg_proposals_idxs = torch.where(labels > 0)[0]\n",
    "            # Get class labels for these positive proposals\n",
    "            fg_cls_labels = labels[fg_proposals_idxs]\n",
    "            \n",
    "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                box_transform_pred[fg_proposals_idxs, fg_cls_labels],\n",
    "                regression_targets[fg_proposals_idxs],\n",
    "                beta=1/9,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            localization_loss = localization_loss / labels.numel()\n",
    "            frcnn_output['frcnn_classification_loss'] = classification_loss\n",
    "            frcnn_output['frcnn_localization_loss'] = localization_loss\n",
    "        \n",
    "        if self.training:\n",
    "            return frcnn_output\n",
    "        else:\n",
    "            device = cls_scores.device\n",
    "            # Apply transformation predictions to proposals\n",
    "            pred_boxes = apply_regression_pred_to_anchors_or_proposals(box_transform_pred, proposals)\n",
    "            pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
    "            \n",
    "            # Clamp box to image boundary\n",
    "            pred_boxes = clamp_boxes_to_image_boundary(pred_boxes, image_shape)\n",
    "            \n",
    "            # create labels for each prediction\n",
    "            pred_labels = torch.arange(num_classes, device=device)\n",
    "            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "            \n",
    "            # remove predictions with the background label\n",
    "            pred_boxes = pred_boxes[:, 1:]\n",
    "            pred_scores = pred_scores[:, 1:]\n",
    "            pred_labels = pred_labels[:, 1:]\n",
    "            \n",
    "            # pred_boxes -> (number_proposals, num_classes-1, 4)\n",
    "            # pred_scores -> (number_proposals, num_classes-1)\n",
    "            # pred_labels -> (number_proposals, num_classes-1)\n",
    "            \n",
    "            # batch everything, by making every class prediction be a separate instance\n",
    "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "            pred_scores = pred_scores.reshape(-1)\n",
    "            pred_labels = pred_labels.reshape(-1)\n",
    "            \n",
    "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n",
    "            frcnn_output['boxes'] = pred_boxes\n",
    "            frcnn_output['scores'] = pred_scores\n",
    "            frcnn_output['labels'] = pred_labels\n",
    "            return frcnn_output\n",
    "    \n",
    "    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n",
    "        r\"\"\"\n",
    "        Method to filter predictions by applying the following in order:\n",
    "        1. Filter low scoring boxes\n",
    "        2. Remove small size boxes∂\n",
    "        3. NMS for each class separately\n",
    "        4. Keep only topK detections\n",
    "        :param pred_boxes:\n",
    "        :param pred_labels:\n",
    "        :param pred_scores:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # remove low scoring boxes\n",
    "        keep = torch.where(pred_scores > self.low_score_threshold)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Remove small boxes\n",
    "        min_size = 16\n",
    "        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Class wise nms\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            curr_indices = torch.where(pred_labels == class_id)[0]\n",
    "            curr_keep_indices = torch.ops.torchvision.nms(pred_boxes[curr_indices],\n",
    "                                                          pred_scores[curr_indices],\n",
    "                                                          self.nms_threshold)\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
    "        keep = post_nms_keep_indices[:self.topK_detections]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, model_config, num_classes):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        self.backbone = vgg16.features[:-1]\n",
    "        self.rpn = RegionProposalNetwork(model_config['backbone_out_channels'],\n",
    "                                         scales=model_config['scales'],\n",
    "                                         aspect_ratios=model_config['aspect_ratios'],\n",
    "                                         model_config=model_config)\n",
    "        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n",
    "        for layer in self.backbone[:10]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.image_mean = [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "        self.min_size = model_config['min_im_size']\n",
    "        self.max_size = model_config['max_im_size']\n",
    "    \n",
    "    def normalize_resize_image_and_boxes(self, image, bboxes):\n",
    "        dtype, device = image.dtype, image.device\n",
    "        \n",
    "        # Normalize\n",
    "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "        #############\n",
    "        \n",
    "        # Resize to 1000x600 such that lowest size dimension is scaled upto 600\n",
    "        # but larger dimension is not more than 1000\n",
    "        # So compute scale factor for both and scale is minimum of these two\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape = torch.tensor(image.shape[-2:])\n",
    "        min_size = torch.min(im_shape).to(dtype=torch.float32)\n",
    "        max_size = torch.max(im_shape).to(dtype=torch.float32)\n",
    "        scale = torch.min(float(self.min_size) / min_size, float(self.max_size) / max_size)\n",
    "        scale_factor = scale.item()\n",
    "        \n",
    "        # Resize image based on scale computed\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image,\n",
    "            size=None,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bilinear\",\n",
    "            recompute_scale_factor=True,\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        if bboxes is not None:\n",
    "            # Resize boxes by\n",
    "            ratios = [\n",
    "                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n",
    "                / torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n",
    "                for s, s_orig in zip(image.shape[-2:], (h, w))\n",
    "            ]\n",
    "            ratio_height, ratio_width = ratios\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n",
    "            xmin = xmin * ratio_width\n",
    "            xmax = xmax * ratio_width\n",
    "            ymin = ymin * ratio_height\n",
    "            ymax = ymax * ratio_height\n",
    "            bboxes = torch.stack((xmin, ymin, xmax, ymax), dim=2)\n",
    "        return image, bboxes\n",
    "    \n",
    "    def forward(self, image, target=None):\n",
    "        old_shape = image.shape[-2:]\n",
    "        if self.training:\n",
    "            # Normalize and resize boxes\n",
    "            image, bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            image, _ = self.normalize_resize_image_and_boxes(image, None)\n",
    "        \n",
    "        # Call backbone\n",
    "        feat = self.backbone(image)\n",
    "        \n",
    "        # Call RPN and get proposals\n",
    "        rpn_output = self.rpn(image, feat, target)\n",
    "        proposals = rpn_output['proposals']\n",
    "        \n",
    "        # Call ROI head and convert proposals to boxes\n",
    "        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n",
    "        if not self.training:\n",
    "            # Transform boxes to original image dimensions called only during inference\n",
    "            frcnn_output['boxes'] = transform_boxes_to_original_size(frcnn_output['boxes'],\n",
    "                                                                     image.shape[-2:],\n",
    "                                                                     old_shape)\n",
    "        return rpn_output, frcnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4501b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33aea4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class PerImageAnnotationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None, annotation_ext=\".txt\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to image files.\n",
    "            annotation_dir (str): Path to per-image annotation files.\n",
    "            transform (callable, optional): Image transforms (should include ToTensor()).\n",
    "            annotation_ext (str): Annotation file extension (e.g., \".txt\").\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform or T.ToTensor()\n",
    "        self.annotation_ext = annotation_ext\n",
    "\n",
    "        self.image_filenames = [f for f in os.listdir(image_dir)\n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        annotation_name = os.path.splitext(image_name)[0] + self.annotation_ext\n",
    "        annotation_path = os.path.join(self.annotation_dir, annotation_name)\n",
    "\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)  # Returns a torch.FloatTensor: [C, H, W]\n",
    "\n",
    "        # Load bounding boxes and class labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue  # skip invalid lines\n",
    "                x1, y1, x2, y2, class_id = map(float, parts)\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(int(class_id))\n",
    "\n",
    "        target = {\n",
    "            'bboxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        return image, target, image_name  # Return image, target, and image name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e588d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_params': {'im_train_path': 'data/train/train_image', 'ann_train_path': 'data/train/train_annotation', 'im_test_path': 'data/test/test_image', 'ann_test_path': 'data/test/test_annotation', 'num_classes': 8}, 'model_params': {'im_channels': 3, 'aspect_ratios': [0.5, 1, 2], 'scales': [128, 256, 512], 'min_im_size': 600, 'max_im_size': 1000, 'backbone_out_channels': 512, 'fc_inner_dim': 1024, 'rpn_bg_threshold': 0.3, 'rpn_fg_threshold': 0.7, 'rpn_nms_threshold': 0.7, 'rpn_train_prenms_topk': 12000, 'rpn_test_prenms_topk': 6000, 'rpn_train_topk': 2000, 'rpn_test_topk': 300, 'rpn_batch_size': 256, 'rpn_pos_fraction': 0.5, 'roi_iou_threshold': 0.5, 'roi_low_bg_iou': 0.0, 'roi_pool_size': 7, 'roi_nms_threshold': 0.3, 'roi_topk_detections': 100, 'roi_score_threshold': 0.05, 'roi_batch_size': 128, 'roi_pos_fraction': 0.25}, 'train_params': {'task_name': 'voc', 'seed': 1111, 'acc_steps': 1, 'num_epochs': 20, 'lr_steps': [12, 16], 'lr': 0.001, 'ckpt_name': 'faster_rcnn_voc2007.pth'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nikos/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 36295/36295 [2:09:36<00:00,  4.67it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\n",
      "RPN Classification Loss : 0.2127 | RPN Localization Loss : 0.1410 | FRCNN Classification Loss : 0.2793 | FRCNN Localization Loss : 0.0467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36295/36295 [2:10:00<00:00,  4.65it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1\n",
      "RPN Classification Loss : 0.1811 | RPN Localization Loss : 0.1285 | FRCNN Classification Loss : 0.2192 | FRCNN Localization Loss : 0.0409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36295/36295 [2:08:55<00:00,  4.69it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 2\n",
      "RPN Classification Loss : 0.1736 | RPN Localization Loss : 0.1253 | FRCNN Classification Loss : 0.2029 | FRCNN Localization Loss : 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36295/36295 [2:06:15<00:00,  4.79it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 3\n",
      "RPN Classification Loss : 0.1694 | RPN Localization Loss : 0.1238 | FRCNN Classification Loss : 0.1936 | FRCNN Localization Loss : 0.0377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 8959/36295 [31:08<1:36:22,  4.73it/s]"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm import tqdm \n",
    "\n",
    "def train(config_path='config.yaml', device='cuda'):\n",
    "    # Read the config file #\n",
    "    with open(config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    ########################\n",
    "    \n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    seed = train_config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    dataset= PerImageAnnotationDataset(config['dataset_params']['im_train_path'],\n",
    "                                      config['dataset_params']['ann_train_path'])\n",
    "    train_dataset = DataLoader(dataset,\n",
    "                               batch_size=1,\n",
    "                               shuffle=True,\n",
    "                               num_workers=4)\n",
    "    \n",
    "    faster_rcnn_model = FasterRCNN(model_config,\n",
    "                                   num_classes=dataset_config['num_classes'])\n",
    "    faster_rcnn_model.train()\n",
    "    faster_rcnn_model.to(device)\n",
    "\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "    optimizer = torch.optim.SGD(lr=train_config['lr'],\n",
    "                                params=filter(lambda p: p.requires_grad,\n",
    "                                              faster_rcnn_model.parameters()),\n",
    "                                weight_decay=5E-4,\n",
    "                                momentum=0.9)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=train_config['lr_steps'], gamma=0.1)\n",
    "    \n",
    "    acc_steps = train_config['acc_steps']\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    step_count = 1\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        rpn_classification_losses = []\n",
    "        rpn_localization_losses = []\n",
    "        frcnn_classification_losses = []\n",
    "        frcnn_localization_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for im, target, fname in tqdm(train_dataset):\n",
    "            im = im.float().to(device)\n",
    "            target['bboxes'] = target['bboxes'].float().to(device)\n",
    "            target['labels'] = target['labels'].long().to(device)\n",
    "            rpn_output, frcnn_output = faster_rcnn_model(im, target)\n",
    "            \n",
    "            rpn_loss = rpn_output['rpn_classification_loss'] + rpn_output['rpn_localization_loss']\n",
    "            frcnn_loss = frcnn_output['frcnn_classification_loss'] + frcnn_output['frcnn_localization_loss']\n",
    "            loss = rpn_loss + frcnn_loss\n",
    "            \n",
    "            rpn_classification_losses.append(rpn_output['rpn_classification_loss'].item())\n",
    "            rpn_localization_losses.append(rpn_output['rpn_localization_loss'].item())\n",
    "            frcnn_classification_losses.append(frcnn_output['frcnn_classification_loss'].item())\n",
    "            frcnn_localization_losses.append(frcnn_output['frcnn_localization_loss'].item())\n",
    "            loss = loss / acc_steps\n",
    "            loss.backward()\n",
    "            if step_count % acc_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            step_count += 1\n",
    "        print('Finished epoch {}'.format(i))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.save(faster_rcnn_model.state_dict(), os.path.join(train_config['task_name'],\n",
    "                                                                train_config['ckpt_name']))\n",
    "        loss_output = ''\n",
    "        loss_output += 'RPN Classification Loss : {:.4f}'.format(np.mean(rpn_classification_losses))\n",
    "        loss_output += ' | RPN Localization Loss : {:.4f}'.format(np.mean(rpn_localization_losses))\n",
    "        loss_output += ' | FRCNN Classification Loss : {:.4f}'.format(np.mean(frcnn_classification_losses))\n",
    "        loss_output += ' | FRCNN Localization Loss : {:.4f}'.format(np.mean(frcnn_localization_losses))\n",
    "        print(loss_output)\n",
    "        scheduler.step()\n",
    "    print('Done Training...')\n",
    "\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba6a94",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Your code runs without errors. The previous cells have set up the environment and device.\n",
    "# You can start building your model or loading data here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298eb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3da390",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
