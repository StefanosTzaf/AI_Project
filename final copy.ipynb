{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cd2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a1a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "zip_path = '../HiXray.zip'\n",
    "\n",
    "# Folder to extract to\n",
    "extract_dir = 'data'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "#Unzip the file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.infolist()  # list of files inside zip\n",
    "\n",
    "        for file in tqdm(file_list, desc=\"Extracting\", unit=\"file\"):\n",
    "            zip_ref.extract(file, path=extract_dir)\n",
    "\n",
    "print(f\"Unzipped to: {extract_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec3a08",
   "metadata": {},
   "source": [
    "All images of HiXray dataset are annotated manually by professional inspectors from an international airport, and the standard of annotating is based on the standard of training security inspectors.\n",
    "\n",
    "HiXray dataset contains a total of 45364 X-ray images(36295 for training, 9069 for testing), including 8 categories of cutters, namely, 'Portable_Charger_1','Portable_Charger_2','Mobile_Phone','Laptop','Tablet','Cosmetic','Water','Nonmetallic_Lighter'.\n",
    "\n",
    "The information structure of annotation file is as follows: image name, category, top-left position of prohibited item (x1, y1), bottom-right position of prohibited item (x2, y2).\n",
    "\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65efc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "from IPython.display import Image as DisplayImage, display\n",
    "\n",
    "image_folder = 'data/train/train_image'\n",
    "txt_folder = 'data/train/train_annotation'\n",
    "\n",
    "# Get list of image files (you can filter by extension)\n",
    "image_files = [f for f in os.listdir(image_folder) ]\n",
    "\n",
    "# Choose a random image\n",
    "random_image = random.choice(image_files)\n",
    "\n",
    "# Create full path for the image and corresponding txt file\n",
    "image_path = os.path.join(image_folder, random_image)\n",
    "txt_file_name = os.path.splitext(random_image)[0] + \".txt\"  # Matching txt file\n",
    "txt_path = os.path.join(txt_folder, txt_file_name)\n",
    "\n",
    "# Read the text file content\n",
    "with open(txt_path, 'r') as file:\n",
    "    txt_content = file.read()\n",
    "print(f\"{image_path}:\\n\")\n",
    "display(DisplayImage(filename=image_path))\n",
    "\n",
    "# Load the image for processing\n",
    "img = PILImage.open(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670dd3e3",
   "metadata": {},
   "source": [
    "The corresponding annotation is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c24fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the text content\n",
    "print(f\"Content of {txt_file_name}:\\n\")\n",
    "print(txt_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25c587",
   "metadata": {},
   "source": [
    "The images are in arbitrary sizes. Therefore we need to resize them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebb813",
   "metadata": {},
   "source": [
    "Fixing the labels in the annotation files so that they have the format: x1 y1 x2 y2 class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ee498",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAP = {\n",
    "   'Portable_Charger_1':0\n",
    "   ,'Portable_Charger_2':1\n",
    "   ,'Mobile_Phone':2,\n",
    "   'Laptop':3,\n",
    "   'Tablet':4,\n",
    "   'Cosmetic':5,\n",
    "   'Water':6,\n",
    "   'Nonmetallic_Lighter':7\n",
    "}\n",
    "\n",
    "def prepare_labels(annotation_dir):\n",
    "# Collect all annotation files\n",
    "  for filename in tqdm(os.listdir(annotation_dir), desc=\"Converting labels\", unit=\"file\"):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(annotation_dir, filename)\n",
    "\n",
    "    new_lines = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 6:\n",
    "                continue  # skip malformed lines\n",
    "            _, class_name, x1, y1, x2, y2 = parts\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            label = CLASS_MAP.get(class_name)\n",
    "            if label is None:\n",
    "                raise ValueError(f\"Unknown class: {class_name}\")\n",
    "            new_lines.append(f\"{x1} {y1} {x2} {y2} {label}\")\n",
    "\n",
    "    # Overwrite the file with new format\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('\\n'.join(new_lines))\n",
    "\n",
    "prepare_labels('data/train/train_annotation')\n",
    "prepare_labels('data/test/test_annotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaf65d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbf385",
   "metadata": {},
   "source": [
    "Roadmap:\n",
    "\n",
    "Training and inference:\n",
    " * Call RPN layers\n",
    " * Generate Anchors\n",
    " * Convert anchors to proposals using Box transformation prediction\n",
    " * Filter Proposals\n",
    "\n",
    "Training only:\n",
    " * Assign Ground Truth boxes to anchors\n",
    " * Compute labels and regression targets for anchors\n",
    " * Sample positive and negative anchors\n",
    " * Compute classification loss using sampled anchors\n",
    " * Compute localization loss using sampled positive anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cee62",
   "metadata": {},
   "source": [
    "ROI head road map:\n",
    "\n",
    "Training:\n",
    "* Assign ground truth boxes to proposals\n",
    "\n",
    "* Sample posotive and negative proposals\n",
    "* Get classification and regression targets for proposals\n",
    "* ROI pooling to get proposal features\n",
    "* Call classification and regression layers\n",
    "* Compute classification and localization loss\n",
    "\n",
    "Inference:\n",
    "* ROI pooling to get proposal features\n",
    "* Classification and regression\n",
    "* Convert proposals to predictions with box transformation prediction\n",
    "* Filter boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b71a0",
   "metadata": {},
   "source": [
    "The Faster RCNN is comprised of a pretrained vgg16 backbone and the regional proposal nn and roi head we implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330644de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4501b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from custom_dataset import PerImageAnnotationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00bd904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_iou(boxes1, boxes2):\n",
    "    r\"\"\"\n",
    "    IOU between two sets of boxes\n",
    "    :param boxes1: (Tensor of shape N x 4)\n",
    "    :param boxes2: (Tensor of shape M x 4)\n",
    "    :return: IOU matrix of shape N x M\n",
    "    \"\"\"\n",
    "    # Area of boxes (x2-x1)*(y2-y1)\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n",
    "    \n",
    "    # Get top left x1,y1 coordinate\n",
    "    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n",
    "    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n",
    "    \n",
    "    # Get bottom right x2,y2 coordinate\n",
    "    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n",
    "    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n",
    "    \n",
    "    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n",
    "    union = area1[:, None] + area2 - intersection_area  # (N, M)\n",
    "    iou = intersection_area / union  # (N, M)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Given all anchor boxes or proposals in image and their respective\n",
    "    ground truth assignments, we use the x1,y1,x2,y2 coordinates of them\n",
    "    to get tx,ty,tw,th transformation targets for all anchor boxes or proposals\n",
    "    :param ground_truth_boxes: (anchors_or_proposals_in_image, 4)\n",
    "        Ground truth box assignments for the anchors/proposals\n",
    "    :param anchors_or_proposals: (anchors_or_proposals_in_image, 4) Anchors/Proposal boxes\n",
    "    :return: regression_targets: (anchors_or_proposals_in_image, 4) transformation targets tx,ty,tw,th\n",
    "        for all anchors/proposal boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for anchors\n",
    "    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n",
    "    \n",
    "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for gt boxes\n",
    "    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
    "    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
    "    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n",
    "    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n",
    "    \n",
    "    targets_dx = (gt_center_x - center_x) / widths\n",
    "    targets_dy = (gt_center_y - center_y) / heights\n",
    "    targets_dw = torch.log(gt_widths / widths)\n",
    "    targets_dh = torch.log(gt_heights / heights)\n",
    "    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
    "    return regression_targets\n",
    "\n",
    "\n",
    "def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Given the transformation parameter predictions for all\n",
    "    input anchors or proposals, transform them accordingly\n",
    "    to generate predicted proposals or predicted boxes\n",
    "    :param box_transform_pred: (num_anchors_or_proposals, num_classes, 4)\n",
    "    :param anchors_or_proposals: (num_anchors_or_proposals, 4)\n",
    "    :return pred_boxes: (num_anchors_or_proposals, num_classes, 4)\n",
    "    \"\"\"\n",
    "    box_transform_pred = box_transform_pred.reshape(\n",
    "        box_transform_pred.size(0), -1, 4)\n",
    "    \n",
    "    # Get cx, cy, w, h from x1,y1,x2,y2\n",
    "    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n",
    "    \n",
    "    dx = box_transform_pred[..., 0]\n",
    "    dy = box_transform_pred[..., 1]\n",
    "    dw = box_transform_pred[..., 2]\n",
    "    dh = box_transform_pred[..., 3]\n",
    "    # dh -> (num_anchors_or_proposals, num_classes)\n",
    "    \n",
    "    # Prevent sending too large values into torch.exp()\n",
    "    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n",
    "    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n",
    "    \n",
    "    pred_center_x = dx * w[:, None] + center_x[:, None]\n",
    "    pred_center_y = dy * h[:, None] + center_y[:, None]\n",
    "    pred_w = torch.exp(dw) * w[:, None]\n",
    "    pred_h = torch.exp(dh) * h[:, None]\n",
    "    # pred_center_x -> (num_anchors_or_proposals, num_classes)\n",
    "    \n",
    "    pred_box_x1 = pred_center_x - 0.5 * pred_w\n",
    "    pred_box_y1 = pred_center_y - 0.5 * pred_h\n",
    "    pred_box_x2 = pred_center_x + 0.5 * pred_w\n",
    "    pred_box_y2 = pred_center_y + 0.5 * pred_h\n",
    "    \n",
    "    pred_boxes = torch.stack((\n",
    "        pred_box_x1,\n",
    "        pred_box_y1,\n",
    "        pred_box_x2,\n",
    "        pred_box_y2),\n",
    "        dim=2)\n",
    "    # pred_boxes -> (num_anchors_or_proposals, num_classes, 4)\n",
    "    return pred_boxes\n",
    "\n",
    "\n",
    "def sample_positive_negative(labels, positive_count, total_count):\n",
    "    # Sample positive and negative proposals\n",
    "    positive = torch.where(labels >= 1)[0]\n",
    "    negative = torch.where(labels == 0)[0]\n",
    "    num_pos = positive_count\n",
    "    num_pos = min(positive.numel(), num_pos)\n",
    "    num_neg = total_count - num_pos\n",
    "    num_neg = min(negative.numel(), num_neg)\n",
    "    perm_positive_idxs = torch.randperm(positive.numel(),\n",
    "                                        device=positive.device)[:num_pos]\n",
    "    perm_negative_idxs = torch.randperm(negative.numel(),\n",
    "                                        device=negative.device)[:num_neg]\n",
    "    pos_idxs = positive[perm_positive_idxs]\n",
    "    neg_idxs = negative[perm_negative_idxs]\n",
    "    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_pos_idx_mask[pos_idxs] = True\n",
    "    sampled_neg_idx_mask[neg_idxs] = True\n",
    "    return sampled_neg_idx_mask, sampled_pos_idx_mask\n",
    "\n",
    "\n",
    "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
    "    boxes_x1 = boxes[..., 0]\n",
    "    boxes_y1 = boxes[..., 1]\n",
    "    boxes_x2 = boxes[..., 2]\n",
    "    boxes_y2 = boxes[..., 3]\n",
    "    height, width = image_shape[-2:]\n",
    "    boxes_x1 = boxes_x1.clamp(min=0, max=width)\n",
    "    boxes_x2 = boxes_x2.clamp(min=0, max=width)\n",
    "    boxes_y1 = boxes_y1.clamp(min=0, max=height)\n",
    "    boxes_y2 = boxes_y2.clamp(min=0, max=height)\n",
    "    boxes = torch.cat((\n",
    "        boxes_x1[..., None],\n",
    "        boxes_y1[..., None],\n",
    "        boxes_x2[..., None],\n",
    "        boxes_y2[..., None]),\n",
    "        dim=-1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def transform_boxes_to_original_size(boxes, new_size, original_size):\n",
    "    r\"\"\"\n",
    "    Boxes are for resized image (min_size=600, max_size=1000).\n",
    "    This method converts the boxes to whatever dimensions\n",
    "    the image was before resizing\n",
    "    :param boxes:\n",
    "    :param new_size:\n",
    "    :param original_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ratios = [\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_height, ratio_width = ratios\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    xmin = xmin * ratio_width\n",
    "    xmax = xmax * ratio_width\n",
    "    ymin = ymin * ratio_height\n",
    "    ymax = ymax * ratio_height\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    r\"\"\"\n",
    "    RPN with following layers on the feature map\n",
    "        1. 3x3 conv layer followed by Relu\n",
    "        2. 1x1 classification conv with num_anchors(num_scales x num_aspect_ratios) output channels\n",
    "        3. 1x1 classification conv with 4 x num_anchors output channels\n",
    "\n",
    "    Classification is done via one value indicating probability of foreground\n",
    "    with sigmoid applied during inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.low_iou_threshold = model_config['rpn_bg_threshold']\n",
    "        self.high_iou_threshold = model_config['rpn_fg_threshold']\n",
    "        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n",
    "        self.rpn_batch_size = model_config['rpn_batch_size']\n",
    "        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n",
    "        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n",
    "        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n",
    "            else model_config['rpn_test_prenms_topk']\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "        \n",
    "        # 3x3 conv layer\n",
    "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 1x1 classification conv layer\n",
    "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
    "        \n",
    "        # 1x1 regression\n",
    "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "        \n",
    "        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n",
    "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
    "            torch.nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "    def generate_anchors(self, image, feat):\n",
    "        r\"\"\"\n",
    "        Method to generate anchors. First we generate one set of zero-centred anchors\n",
    "        using the scales and aspect ratios provided.\n",
    "        We then generate shift values in x,y axis for all featuremap locations.\n",
    "        The single zero centred anchors generated are replicated and shifted accordingly\n",
    "        to generate anchors for all feature map locations.\n",
    "        Note that these anchors are generated such that their centre is top left corner of the\n",
    "        feature map cell rather than the centre of the feature map cell.\n",
    "        :param image: (N, C, H, W) tensor\n",
    "        :param feat: (N, C_feat, H_feat, W_feat) tensor\n",
    "        :return: anchor boxes of shape (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        \"\"\"\n",
    "        grid_h, grid_w = feat.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "        \n",
    "        # For the vgg16 case stride would be 16 for both h and w\n",
    "        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n",
    "        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n",
    "        \n",
    "        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
    "        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
    "        \n",
    "        # Assuming anchors of scale 128 sq pixels\n",
    "        # For 1:1 it would be (128, 128) -> area=16384\n",
    "        # For 2:1 it would be (181.02, 90.51) -> area=16384\n",
    "        # For 1:2 it would be (90.51, 181.02) -> area=16384\n",
    "        \n",
    "        # The below code ensures h/w = aspect_ratios and h*w=1\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "        \n",
    "        # Now we will just multiply h and w with scale(example 128)\n",
    "        # to make h*w = 128 sq pixels and h/w = aspect_ratios\n",
    "        # This gives us the widths and heights of all anchors\n",
    "        # which we need to replicate at all locations\n",
    "        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        \n",
    "        # Now we make all anchors zero centred\n",
    "        # So x1, y1, x2, y2 = -w/2, -h/2, w/2, h/2\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "        base_anchors = base_anchors.round()\n",
    "        \n",
    "        # Get the shifts in x axis (0, 1,..., W_feat-1) * stride_w\n",
    "        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
    "\n",
    "        # Get the shifts in x axis (0, 1,..., H_feat-1) * stride_h\n",
    "        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
    "        \n",
    "        # Create a grid using these shifts\n",
    "        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
    "        # shifts_x -> (H_feat, W_feat)\n",
    "        # shifts_y -> (H_feat, W_feat)\n",
    "        \n",
    "        shifts_x = shifts_x.reshape(-1)\n",
    "        shifts_y = shifts_y.reshape(-1)\n",
    "        # Setting shifts for x1 and x2(same as shifts_x) and y1 and y2(same as shifts_y)\n",
    "        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        \n",
    "        # base_anchors -> (num_anchors_per_location, 4)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        # Add these shifts to each of the base anchors\n",
    "        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n",
    "        # anchors -> (H_feat * W_feat, num_anchors_per_location, 4)\n",
    "        anchors = anchors.reshape(-1, 4)\n",
    "        # anchors -> (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        return anchors\n",
    "    \n",
    "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "        r\"\"\"\n",
    "        For each anchor assign a ground truth box based on the IOU.\n",
    "        Also creates classification labels to be used for training\n",
    "        label=1 for anchors where maximum IOU with a gtbox > high_iou_threshold\n",
    "        label=0 for anchors where maximum IOU with a gtbox < low_iou_threshold\n",
    "        label=-1 for anchors where maximum IOU with a gtbox between (low_iou_threshold, high_iou_threshold)\n",
    "        :param anchors: (num_anchors_in_image, 4) all anchor boxes\n",
    "        :param gt_boxes: (num_gt_boxes_in_image, 4) all ground truth boxes\n",
    "        :return:\n",
    "            label: (num_anchors_in_image) {-1/0/1}\n",
    "            matched_gt_boxes: (num_anchors_in_image, 4) coordinates of assigned gt_box to each anchor\n",
    "                Even background/to_be_ignored anchors will be assigned some ground truth box.\n",
    "                It's fine, we will use label to differentiate those instances later\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get (gt_boxes, num_anchors_in_image) IOU matrix\n",
    "        iou_matrix = get_iou(gt_boxes, anchors)\n",
    "        \n",
    "        # For each anchor get the gt box index with maximum overlap\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        # best_match_gt_idx -> (num_anchors_in_image)\n",
    "        \n",
    "        # This copy of best_match_gt_idx will be needed later to\n",
    "        # add low quality matches\n",
    "        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n",
    "        \n",
    "        # Based on threshold, update the values of best_match_gt_idx\n",
    "        # For anchors with highest IOU < low_threshold update to be -1\n",
    "        # For anchors with highest IOU between low_threshold & high threshold update to be -2\n",
    "        below_low_threshold = best_match_iou < self.low_iou_threshold\n",
    "        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n",
    "        best_match_gt_idx[below_low_threshold] = -1\n",
    "        best_match_gt_idx[between_thresholds] = -2\n",
    "        \n",
    "        # Add low quality anchor boxes, if for a given ground truth box, these are the ones\n",
    "        # that have highest IOU with that gt box\n",
    "        \n",
    "        # For each gt box, get the maximum IOU value amongst all anchors\n",
    "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        # best_anchor_iou_for_gt -> (num_gt_boxes_in_image)\n",
    "        \n",
    "        # For each gt box get those anchors\n",
    "        # which have this same IOU as present in best_anchor_iou_for_gt\n",
    "        # This is to ensure if 10 anchors all have the same IOU value,\n",
    "        # which is equal to the highest IOU that this gt box has with any anchor\n",
    "        # then we get all these 10 anchors\n",
    "        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "        # gt_pred_pair_with_highest_iou -> [0, 0, 0, 1, 1, 1], [8896,  8905,  8914, 10472, 10805, 11138]\n",
    "        # This means that anchors at the first 3 indexes have an IOU with gt box at index 0\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # Similarly anchor at last three indexes(10472, 10805, 11138) have an IOU with gt box at index 1\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # These 6 anchor indexes will also be added as positive anchors\n",
    "        \n",
    "        # Get all the anchors indexes to update\n",
    "        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n",
    "        \n",
    "        # Update the matched gt index for all these anchors with whatever was the best gt box\n",
    "        # prior to thresholding\n",
    "        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n",
    "        \n",
    "        # best_match_gt_idx is either a valid index for all anchors or -1(background) or -2(to be ignored)\n",
    "        # Clamp this so that the best_match_gt_idx is a valid non-negative index\n",
    "        # At this moment the -1 and -2 labelled anchors will be mapped to the 0th gt box\n",
    "        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Set all foreground anchor labels as 1\n",
    "        labels = best_match_gt_idx >= 0\n",
    "        labels = labels.to(dtype=torch.float32)\n",
    "        \n",
    "        # Set all background anchor labels as 0\n",
    "        background_anchors = best_match_gt_idx == -1\n",
    "        labels[background_anchors] = 0.0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1\n",
    "        ignored_anchors = best_match_gt_idx == -2\n",
    "        labels[ignored_anchors] = -1.0\n",
    "        # Later for classification we will only pick labels which have > 0 label\n",
    "        \n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "        r\"\"\"\n",
    "        This method does three kinds of filtering/modifications\n",
    "        1. Pre NMS topK filtering\n",
    "        2. Make proposals valid by clamping coordinates(0, width/height)\n",
    "        2. Small Boxes filtering based on width and height\n",
    "        3. NMS\n",
    "        4. Post NMS topK filtering\n",
    "        :param proposals: (num_anchors_in_image, 4)\n",
    "        :param cls_scores: (num_anchors_in_image, 4) these are cls logits\n",
    "        :param image_shape: resized image shape needed to clip proposals to image boundary\n",
    "        :return: proposals and cls_scores: (num_filtered_proposals, 4) and (num_filtered_proposals)\n",
    "        \"\"\"\n",
    "        # Pre NMS Filtering\n",
    "        cls_scores = cls_scores.reshape(-1)\n",
    "        cls_scores = torch.sigmoid(cls_scores)\n",
    "        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n",
    "        \n",
    "        cls_scores = cls_scores[top_n_idx]\n",
    "        proposals = proposals[top_n_idx]\n",
    "        ##################\n",
    "        \n",
    "        # Clamp boxes to image boundary\n",
    "        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n",
    "        ####################\n",
    "        \n",
    "        # Small boxes based on width and height filtering\n",
    "        min_size = 16\n",
    "        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        proposals = proposals[keep]\n",
    "        cls_scores = cls_scores[keep]\n",
    "        ####################\n",
    "        \n",
    "        # NMS based on objectness scores\n",
    "        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n",
    "        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n",
    "        keep_mask[keep_indices] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        # Sort by objectness\n",
    "        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
    "        \n",
    "        # Post NMS topk filtering\n",
    "        proposals, cls_scores = (proposals[post_nms_keep_indices[:self.rpn_topk]],\n",
    "                                 cls_scores[post_nms_keep_indices[:self.rpn_topk]])\n",
    "        \n",
    "        return proposals, cls_scores\n",
    "    \n",
    "    def forward(self, image, feat, target=None):\n",
    "        r\"\"\"\n",
    "        Main method for RPN does the following:\n",
    "        1. Call RPN specific conv layers to generate classification and\n",
    "            bbox transformation predictions for anchors\n",
    "        2. Generate anchors for entire image\n",
    "        3. Transform generated anchors based on predicted bbox transformation to generate proposals\n",
    "        4. Filter proposals\n",
    "        5. For training additionally we do the following:\n",
    "            a. Assign target ground truth labels and boxes to each anchors\n",
    "            b. Sample positive and negative anchors\n",
    "            c. Compute classification loss using sampled pos/neg anchors\n",
    "            d. Compute Localization loss using sampled pos anchors\n",
    "        :param image:\n",
    "        :param feat:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Call RPN layers\n",
    "        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n",
    "        cls_scores = self.cls_layer(rpn_feat)\n",
    "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "        # Generate anchors\n",
    "        anchors = self.generate_anchors(image, feat)\n",
    "        \n",
    "        # Reshape classification scores to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 1)\n",
    "        # cls_score -> (Batch_Size, Number of Anchors per location, H_feat, W_feat)\n",
    "        number_of_anchors_per_location = cls_scores.size(1)\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1)\n",
    "        cls_scores = cls_scores.reshape(-1, 1)\n",
    "        # cls_score -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 1)\n",
    "        \n",
    "        # Reshape bbox predictions to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 4)\n",
    "        # box_transform_pred -> (Batch_Size, Number of Anchors per location*4, H_feat, W_feat)\n",
    "        box_transform_pred = box_transform_pred.view(\n",
    "            box_transform_pred.size(0),\n",
    "            number_of_anchors_per_location,\n",
    "            4,\n",
    "            rpn_feat.shape[-2],\n",
    "            rpn_feat.shape[-1])\n",
    "        box_transform_pred = box_transform_pred.permute(0, 3, 4, 1, 2)\n",
    "        box_transform_pred = box_transform_pred.reshape(-1, 4)\n",
    "        # box_transform_pred -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 4)\n",
    "        \n",
    "        # Transform generated anchors according to box transformation prediction\n",
    "        proposals = apply_regression_pred_to_anchors_or_proposals(\n",
    "            box_transform_pred.detach().reshape(-1, 1, 4),\n",
    "            anchors)\n",
    "        proposals = proposals.reshape(proposals.size(0), 4)\n",
    "        ######################\n",
    "        \n",
    "        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n",
    "        rpn_output = {\n",
    "            'proposals': proposals,\n",
    "            'scores': scores\n",
    "        }\n",
    "        if not self.training or target is None:\n",
    "            # If we are not training no need to do anything\n",
    "            return rpn_output\n",
    "        else:\n",
    "            # Assign gt box and label for each anchor\n",
    "            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n",
    "                anchors,\n",
    "                target['bboxes'][0])\n",
    "            \n",
    "            # Based on gt assignment above, get regression target for the anchors\n",
    "            # matched_gt_boxes_for_anchors -> (Number of anchors in image, 4)\n",
    "            # anchors -> (Number of anchors in image, 4)\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n",
    "            \n",
    "            ####### Sampling positive and negative anchors ####\n",
    "            # Our labels were {fg:1, bg:0, to_be_ignored:-1}\n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
    "                labels_for_anchors,\n",
    "                positive_count=self.rpn_pos_count,\n",
    "                total_count=self.rpn_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            localization_loss = (\n",
    "                    torch.nn.functional.smooth_l1_loss(\n",
    "                        box_transform_pred[sampled_pos_idx_mask],\n",
    "                        regression_targets[sampled_pos_idx_mask],\n",
    "                        beta=1 / 9,\n",
    "                        reduction=\"sum\",\n",
    "                    )\n",
    "                    / (sampled_idxs.numel())\n",
    "            ) \n",
    "\n",
    "            cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(cls_scores[sampled_idxs].flatten(),\n",
    "                                                                            labels_for_anchors[sampled_idxs].flatten())\n",
    "\n",
    "            rpn_output['rpn_classification_loss'] = cls_loss\n",
    "            rpn_output['rpn_localization_loss'] = localization_loss\n",
    "            return rpn_output\n",
    "\n",
    "\n",
    "class ROIHead(nn.Module):\n",
    "    r\"\"\"\n",
    "    ROI head on top of ROI pooling layer for generating\n",
    "    classification and box transformation predictions\n",
    "    We have two fc layers followed by a classification fc layer\n",
    "    and a bbox regression fc layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_config, num_classes, in_channels):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.roi_batch_size = model_config['roi_batch_size']\n",
    "        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n",
    "        self.iou_threshold = model_config['roi_iou_threshold']\n",
    "        self.low_bg_iou = model_config['roi_low_bg_iou']\n",
    "        self.nms_threshold = model_config['roi_nms_threshold']\n",
    "        self.topK_detections = model_config['roi_topk_detections']\n",
    "        self.low_score_threshold = model_config['roi_score_threshold']\n",
    "        self.pool_size = model_config['roi_pool_size']\n",
    "        self.fc_inner_dim = model_config['fc_inner_dim']\n",
    "        \n",
    "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
    "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
    "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
    "        \n",
    "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
    "        torch.nn.init.constant_(self.cls_layer.bias, 0)\n",
    "\n",
    "        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n",
    "        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n",
    "    \n",
    "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        r\"\"\"\n",
    "        Given a set of proposals and ground truth boxes and their respective labels.\n",
    "        Use IOU to assign these proposals to some gt box or background\n",
    "        :param proposals: (number_of_proposals, 4)\n",
    "        :param gt_boxes: (number_of_gt_boxes, 4)\n",
    "        :param gt_labels: (number_of_gt_boxes)\n",
    "        :return:\n",
    "            labels: (number_of_proposals)\n",
    "            matched_gt_boxes: (number_of_proposals, 4)\n",
    "        \"\"\"\n",
    "        # Get IOU Matrix between gt boxes and proposals\n",
    "        iou_matrix = get_iou(gt_boxes, proposals)\n",
    "        # For each gt box proposal find best matching gt box\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n",
    "        ignored_proposals = best_match_iou < self.low_bg_iou\n",
    "        \n",
    "        # Update best match of low IOU proposals to -1\n",
    "        best_match_gt_idx[background_proposals] = -1\n",
    "        best_match_gt_idx[ignored_proposals] = -2\n",
    "        \n",
    "        # Get best marching gt boxes for ALL proposals\n",
    "        # Even background proposals would have a gt box assigned to it\n",
    "        # Label will be used to ignore them later\n",
    "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Get class label for all proposals according to matching gt boxes\n",
    "        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n",
    "        labels = labels.to(dtype=torch.int64)\n",
    "        \n",
    "        # Update background proposals to be of label 0(background)\n",
    "        labels[background_proposals] = 0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1(will be ignored)\n",
    "        labels[ignored_proposals] = -1\n",
    "        \n",
    "        return labels, matched_gt_boxes_for_proposals\n",
    "    \n",
    "    def forward(self, feat, proposals, image_shape, target):\n",
    "        r\"\"\"\n",
    "        Main method for ROI head that does the following:\n",
    "        1. If training assign target boxes and labels to all proposals\n",
    "        2. If training sample positive and negative proposals\n",
    "        3. If training get bbox transformation targets for all proposals based on assignments\n",
    "        4. Get ROI Pooled features for all proposals\n",
    "        5. Call fc6, fc7 and classification and bbox transformation fc layers\n",
    "        6. Compute classification and localization loss\n",
    "\n",
    "        :param feat:\n",
    "        :param proposals:\n",
    "        :param image_shape:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.training and target is not None:\n",
    "            # Add ground truth to proposals\n",
    "            proposals = torch.cat([proposals, target['bboxes'][0]], dim=0)\n",
    "            \n",
    "            gt_boxes = target['bboxes'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "            \n",
    "            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "            \n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(labels,\n",
    "                                                                                  positive_count=self.roi_pos_count,\n",
    "                                                                                  total_count=self.roi_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            # Keep only sampled proposals\n",
    "            proposals = proposals[sampled_idxs]\n",
    "            labels = labels[sampled_idxs]\n",
    "            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_proposals, proposals)\n",
    "            # regression_targets -> (sampled_training_proposals, 4)\n",
    "            # matched_gt_boxes_for_proposals -> (sampled_training_proposals, 4)\n",
    "        \n",
    "        # Get desired scale to pass to roi pooling function\n",
    "        # For vgg16 case this would be 1/16 (0.0625)\n",
    "        size = feat.shape[-2:]\n",
    "        possible_scales = []\n",
    "        for s1, s2 in zip(size, image_shape):\n",
    "            approx_scale = float(s1) / float(s2)\n",
    "            scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n",
    "            possible_scales.append(scale)\n",
    "        assert possible_scales[0] == possible_scales[1]\n",
    "        \n",
    "        # ROI pooling and call all layers for prediction\n",
    "        proposal_roi_pool_feats = torchvision.ops.roi_pool(feat, [proposals],\n",
    "                                                           output_size=self.pool_size,\n",
    "                                                           spatial_scale=possible_scales[0])\n",
    "        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
    "        box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_pool_feats))\n",
    "        box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n",
    "        cls_scores = self.cls_layer(box_fc_7)\n",
    "        box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
    "        # cls_scores -> (proposals, num_classes)\n",
    "        # box_transform_pred -> (proposals, num_classes * 4)\n",
    "        ##############################################\n",
    "        \n",
    "        num_boxes, num_classes = cls_scores.shape\n",
    "        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n",
    "        frcnn_output = {}\n",
    "        if self.training and target is not None:\n",
    "            classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n",
    "            \n",
    "            # Compute localization loss only for non-background labelled proposals\n",
    "            fg_proposals_idxs = torch.where(labels > 0)[0]\n",
    "            # Get class labels for these positive proposals\n",
    "            fg_cls_labels = labels[fg_proposals_idxs]\n",
    "            \n",
    "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                box_transform_pred[fg_proposals_idxs, fg_cls_labels],\n",
    "                regression_targets[fg_proposals_idxs],\n",
    "                beta=1/9,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            localization_loss = localization_loss / labels.numel()\n",
    "            frcnn_output['frcnn_classification_loss'] = classification_loss\n",
    "            frcnn_output['frcnn_localization_loss'] = localization_loss\n",
    "        \n",
    "        if self.training:\n",
    "            return frcnn_output\n",
    "        else:\n",
    "            device = cls_scores.device\n",
    "            # Apply transformation predictions to proposals\n",
    "            pred_boxes = apply_regression_pred_to_anchors_or_proposals(box_transform_pred, proposals)\n",
    "            pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
    "            \n",
    "            # Clamp box to image boundary\n",
    "            pred_boxes = clamp_boxes_to_image_boundary(pred_boxes, image_shape)\n",
    "            \n",
    "            # create labels for each prediction\n",
    "            pred_labels = torch.arange(num_classes, device=device)\n",
    "            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "            \n",
    "            # remove predictions with the background label\n",
    "            pred_boxes = pred_boxes[:, 1:]\n",
    "            pred_scores = pred_scores[:, 1:]\n",
    "            pred_labels = pred_labels[:, 1:]\n",
    "            \n",
    "            # pred_boxes -> (number_proposals, num_classes-1, 4)\n",
    "            # pred_scores -> (number_proposals, num_classes-1)\n",
    "            # pred_labels -> (number_proposals, num_classes-1)\n",
    "            \n",
    "            # batch everything, by making every class prediction be a separate instance\n",
    "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "            pred_scores = pred_scores.reshape(-1)\n",
    "            pred_labels = pred_labels.reshape(-1)\n",
    "            \n",
    "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n",
    "            frcnn_output['boxes'] = pred_boxes\n",
    "            frcnn_output['scores'] = pred_scores\n",
    "            frcnn_output['labels'] = pred_labels\n",
    "            return frcnn_output\n",
    "    \n",
    "    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n",
    "        r\"\"\"\n",
    "        Method to filter predictions by applying the following in order:\n",
    "        1. Filter low scoring boxes\n",
    "        2. Remove small size boxes∂\n",
    "        3. NMS for each class separately\n",
    "        4. Keep only topK detections\n",
    "        :param pred_boxes:\n",
    "        :param pred_labels:\n",
    "        :param pred_scores:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # remove low scoring boxes\n",
    "        keep = torch.where(pred_scores > self.low_score_threshold)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Remove small boxes\n",
    "        min_size = 16\n",
    "        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Class wise nms\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            curr_indices = torch.where(pred_labels == class_id)[0]\n",
    "            curr_keep_indices = torch.ops.torchvision.nms(pred_boxes[curr_indices],\n",
    "                                                          pred_scores[curr_indices],\n",
    "                                                          self.nms_threshold)\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
    "        keep = post_nms_keep_indices[:self.topK_detections]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, model_config, num_classes):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        self.backbone = vgg16.features[:-1]\n",
    "        self.rpn = RegionProposalNetwork(model_config['backbone_out_channels'],\n",
    "                                         scales=model_config['scales'],\n",
    "                                         aspect_ratios=model_config['aspect_ratios'],\n",
    "                                         model_config=model_config)\n",
    "        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n",
    "        for layer in self.backbone[:10]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.image_mean = [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "        self.min_size = model_config['min_im_size']\n",
    "        self.max_size = model_config['max_im_size']\n",
    "    \n",
    "    def normalize_resize_image_and_boxes(self, image, bboxes):\n",
    "        dtype, device = image.dtype, image.device\n",
    "        \n",
    "        # Normalize\n",
    "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "        #############\n",
    "        \n",
    "        # Resize to 1000x600 such that lowest size dimension is scaled upto 600\n",
    "        # but larger dimension is not more than 1000\n",
    "        # So compute scale factor for both and scale is minimum of these two\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape = torch.tensor(image.shape[-2:])\n",
    "        min_size = torch.min(im_shape).to(dtype=torch.float32)\n",
    "        max_size = torch.max(im_shape).to(dtype=torch.float32)\n",
    "        scale = torch.min(float(self.min_size) / min_size, float(self.max_size) / max_size)\n",
    "        scale_factor = scale.item()\n",
    "        \n",
    "        # Resize image based on scale computed\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image,\n",
    "            size=None,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bilinear\",\n",
    "            recompute_scale_factor=True,\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        if bboxes is not None:\n",
    "            # Resize boxes by\n",
    "            ratios = [\n",
    "                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n",
    "                / torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n",
    "                for s, s_orig in zip(image.shape[-2:], (h, w))\n",
    "            ]\n",
    "            ratio_height, ratio_width = ratios\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n",
    "            xmin = xmin * ratio_width\n",
    "            xmax = xmax * ratio_width\n",
    "            ymin = ymin * ratio_height\n",
    "            ymax = ymax * ratio_height\n",
    "            bboxes = torch.stack((xmin, ymin, xmax, ymax), dim=2)\n",
    "        return image, bboxes\n",
    "    \n",
    "    def forward(self, image, target=None):\n",
    "        old_shape = image.shape[-2:]\n",
    "        if self.training:\n",
    "            # Normalize and resize boxes\n",
    "            image, bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            image, _ = self.normalize_resize_image_and_boxes(image, None)\n",
    "        \n",
    "        # Call backbone\n",
    "        feat = self.backbone(image)\n",
    "        \n",
    "        # Call RPN and get proposals\n",
    "        rpn_output = self.rpn(image, feat, target)\n",
    "        proposals = rpn_output['proposals']\n",
    "        \n",
    "        # Call ROI head and convert proposals to boxes\n",
    "        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n",
    "        if not self.training:\n",
    "            # Transform boxes to original image dimensions called only during inference\n",
    "            frcnn_output['boxes'] = transform_boxes_to_original_size(frcnn_output['boxes'],\n",
    "                                                                     image.shape[-2:],\n",
    "                                                                     old_shape)\n",
    "        return rpn_output, frcnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e588d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_params': {'im_train_path': 'data/train/train_image', 'ann_train_path': 'data/train/train_annotation', 'im_test_path': 'data/test/test_image', 'ann_test_path': 'data/test/test_annotation', 'num_classes': 8}, 'model_params': {'im_channels': 3, 'aspect_ratios': [0.5, 1, 2], 'scales': [128, 256, 512], 'min_im_size': 600, 'max_im_size': 1000, 'backbone_out_channels': 512, 'fc_inner_dim': 1024, 'rpn_bg_threshold': 0.3, 'rpn_fg_threshold': 0.7, 'rpn_nms_threshold': 0.7, 'rpn_train_prenms_topk': 12000, 'rpn_test_prenms_topk': 6000, 'rpn_train_topk': 2000, 'rpn_test_topk': 300, 'rpn_batch_size': 256, 'rpn_pos_fraction': 0.5, 'roi_iou_threshold': 0.5, 'roi_low_bg_iou': 0.0, 'roi_pool_size': 7, 'roi_nms_threshold': 0.3, 'roi_topk_detections': 100, 'roi_score_threshold': 0.05, 'roi_batch_size': 128, 'roi_pos_fraction': 0.25}, 'train_params': {'task_name': 'xray_faster_rcnn', 'seed': 1111, 'acc_steps': 1, 'num_epochs': 10, 'lr_steps': [12, 16], 'lr': 0.001, 'ckpt_name': 'faster_rcnn.pth'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nikos/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 10/36295 [00:03<3:12:01,  3.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 94\u001b[0m\n\u001b[1;32m     89\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone Training...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m train()\n\u001b[1;32m     95\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(faster_rcnn_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config_path, device)\u001b[0m\n\u001b[1;32m     60\u001b[0m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     61\u001b[0m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 62\u001b[0m rpn_output, frcnn_output \u001b[38;5;241m=\u001b[39m faster_rcnn_model(im, target)\n\u001b[1;32m     64\u001b[0m rpn_loss \u001b[38;5;241m=\u001b[39m rpn_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrpn_classification_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m rpn_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrpn_localization_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     65\u001b[0m frcnn_loss \u001b[38;5;241m=\u001b[39m frcnn_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrcnn_classification_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m frcnn_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrcnn_localization_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 820\u001b[0m, in \u001b[0;36mFasterRCNN.forward\u001b[0;34m(self, image, target)\u001b[0m\n\u001b[1;32m    817\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(image)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# Call RPN and get proposals\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m rpn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(image, feat, target)\n\u001b[1;32m    821\u001b[0m proposals \u001b[38;5;241m=\u001b[39m rpn_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# Call ROI head and convert proposals to boxes\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 442\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, image, feat, target)\u001b[0m\n\u001b[1;32m    439\u001b[0m box_transform_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_reg_layer(rpn_feat)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# Generate anchors\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_anchors(image, feat)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Reshape classification scores to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 1)\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# cls_score -> (Batch_Size, Number of Anchors per location, H_feat, W_feat)\u001b[39;00m\n\u001b[1;32m    446\u001b[0m number_of_anchors_per_location \u001b[38;5;241m=\u001b[39m cls_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 234\u001b[0m, in \u001b[0;36mRegionProposalNetwork.generate_anchors\u001b[0;34m(self, image, feat)\u001b[0m\n\u001b[1;32m    231\u001b[0m image_h, image_w \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# For the vgg16 case stride would be 16 for both h and w\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m stride_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(image_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m grid_h, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mfeat\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    235\u001b[0m stride_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(image_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m grid_w, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mfeat\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    237\u001b[0m scales \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales, dtype\u001b[38;5;241m=\u001b[39mfeat\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mfeat\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm import tqdm \n",
    "\n",
    "def train(config_path='config.yaml', device='cuda'):\n",
    "    # Read the config file #\n",
    "    with open(config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    ########################\n",
    "    \n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    seed = train_config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    dataset= PerImageAnnotationDataset(config['dataset_params']['im_train_path'],\n",
    "                                      config['dataset_params']['ann_train_path'])\n",
    "    train_dataset = DataLoader(dataset,\n",
    "                               batch_size=1,\n",
    "                               shuffle=True,\n",
    "                               num_workers=4)\n",
    "    \n",
    "    \n",
    "    faster_rcnn_model = FasterRCNN(model_config,\n",
    "                                   num_classes=dataset_config['num_classes'])\n",
    "    faster_rcnn_model.train()\n",
    "    faster_rcnn_model.to(device)\n",
    "\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "    optimizer = torch.optim.SGD(lr=train_config['lr'],\n",
    "                                params=filter(lambda p: p.requires_grad,\n",
    "                                              faster_rcnn_model.parameters()),\n",
    "                                weight_decay=5E-4,\n",
    "                                momentum=0.9)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=train_config['lr_steps'], gamma=0.1)\n",
    "    \n",
    "    acc_steps = train_config['acc_steps']\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    step_count = 1\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        rpn_classification_losses = []\n",
    "        rpn_localization_losses = []\n",
    "        frcnn_classification_losses = []\n",
    "        frcnn_localization_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for im, target, fname in tqdm(train_dataset):\n",
    "            im = im.float().to(device)\n",
    "            target['bboxes'] = target['bboxes'].float().to(device)\n",
    "            target['labels'] = target['labels'].long().to(device)\n",
    "            rpn_output, frcnn_output = faster_rcnn_model(im, target)\n",
    "            \n",
    "            rpn_loss = rpn_output['rpn_classification_loss'] + rpn_output['rpn_localization_loss']\n",
    "            frcnn_loss = frcnn_output['frcnn_classification_loss'] + frcnn_output['frcnn_localization_loss']\n",
    "            loss = rpn_loss + frcnn_loss\n",
    "            \n",
    "            rpn_classification_losses.append(rpn_output['rpn_classification_loss'].item())\n",
    "            rpn_localization_losses.append(rpn_output['rpn_localization_loss'].item())\n",
    "            frcnn_classification_losses.append(frcnn_output['frcnn_classification_loss'].item())\n",
    "            frcnn_localization_losses.append(frcnn_output['frcnn_localization_loss'].item())\n",
    "            loss = loss / acc_steps\n",
    "            loss.backward()\n",
    "            if step_count % acc_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            step_count += 1\n",
    "        print('Finished epoch {}'.format(i))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.save(faster_rcnn_model.state_dict(), os.path.join(train_config['task_name'],\n",
    "                                                                train_config['ckpt_name']))\n",
    "        loss_output = ''\n",
    "        loss_output += 'RPN Classification Loss : {:.4f}'.format(np.mean(rpn_classification_losses))\n",
    "        loss_output += ' | RPN Localization Loss : {:.4f}'.format(np.mean(rpn_localization_losses))\n",
    "        loss_output += ' | FRCNN Classification Loss : {:.4f}'.format(np.mean(frcnn_classification_losses))\n",
    "        loss_output += ' | FRCNN Localization Loss : {:.4f}'.format(np.mean(frcnn_localization_losses))\n",
    "        print(loss_output)\n",
    "        scheduler.step()\n",
    "    print('Done Training...')\n",
    "\n",
    "\n",
    "\n",
    "train()\n",
    "torch.save(faster_rcnn_model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba6a94",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Your code runs without errors. The previous cells have set up the environment and device.\n",
    "# You can start building your model or loading data here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
